# Glossario dei Termini Tecnici di NLP

Questo glossario raccoglie i principali termini tecnici utilizzati nel campo del Natural Language Processing (NLP), con definizioni chiare e concise per facilitare la comprensione dei concetti presentati nel corso.

## A

**Algoritmo**: Una procedura o formula per risolvere un problema, basata su una sequenza finita di istruzioni.

**Analisi del sentiment**: Tecnica di NLP che identifica e estrae opinioni soggettive da testi, classificando la polarità emotiva (positiva, negativa o neutra).

**Analisi sintattica**: Processo di analisi della struttura grammaticale di una frase, identificando le relazioni tra le parole.

**Apprendimento automatico (Machine Learning)**: Branca dell'intelligenza artificiale che permette ai computer di apprendere dai dati senza essere esplicitamente programmati.

**Apprendimento profondo (Deep Learning)**: Sottocampo del machine learning basato su reti neurali artificiali con molteplici livelli (deep neural networks).

**Apprendimento supervisionato**: Tecnica di machine learning in cui l'algoritmo viene addestrato su un dataset etichettato.

**Apprendimento non supervisionato**: Tecnica di machine learning in cui l'algoritmo cerca di identificare pattern in dati non etichettati.

**ARIMA (AutoRegressive Integrated Moving Average)**: Modello statistico per l'analisi e la previsione di serie temporali.

**Attenzione (Attention)**: Meccanismo che permette ai modelli di concentrarsi su parti specifiche dell'input durante l'elaborazione.

## B

**Bag of Words (BoW)**: Rappresentazione del testo che considera la frequenza delle parole ma ignora l'ordine e la struttura grammaticale.

**BERT (Bidirectional Encoder Representations from Transformers)**: Modello di linguaggio pre-addestrato che considera il contesto bidirezionale delle parole.

**Bigram**: Sequenza di due elementi adiacenti da un testo (solitamente parole).

**Bitext**: Corpus parallelo di testi in due lingue, utilizzato per l'addestramento di sistemi di traduzione automatica.

## C

**Chatbot**: Sistema software progettato per simulare conversazioni con utenti umani, spesso utilizzando tecniche di NLP.

**Classificazione del testo**: Processo di assegnazione di categorie o etichette a documenti testuali.

**Clustering**: Tecnica di apprendimento non supervisionato che raggruppa dati simili.

**Corpus**: Raccolta strutturata di testi utilizzata per l'analisi linguistica e l'addestramento di modelli NLP.

**Co-occorrenza**: Frequenza con cui due termini appaiono insieme in un corpus.

## D

**Disambiguazione del senso delle parole (Word Sense Disambiguation)**: Processo di identificazione del significato corretto di una parola in un contesto specifico.

**Distanza di Levenshtein**: Misura della differenza tra due stringhe, calcolata come il numero minimo di operazioni necessarie per trasformare una stringa nell'altra.

**Distributional Semantics**: Approccio che deriva il significato delle parole dalla loro distribuzione statistica nei testi.

## E

**Elaborazione del linguaggio naturale (NLP)**: Campo dell'intelligenza artificiale che si occupa dell'interazione tra computer e linguaggio umano.

**Embedding**: Rappresentazione di parole, frasi o documenti come vettori numerici in uno spazio multidimensionale.

**Entità nominata (Named Entity)**: Parola o frase che identifica chiaramente un elemento come una persona, un'organizzazione, un luogo, ecc.

**Estrazione di informazioni**: Processo di identificazione automatica di informazioni strutturate da testi non strutturati.

## F

**Feature Engineering**: Processo di selezione e trasformazione delle variabili utilizzate per addestrare un modello di machine learning.

**Fine-tuning**: Processo di adattamento di un modello pre-addestrato a un compito specifico.

**F1-Score**: Media armonica di precisione e recall, utilizzata per valutare le prestazioni di un classificatore.

## G

**GPT (Generative Pre-trained Transformer)**: Famiglia di modelli di linguaggio basati sull'architettura Transformer, addestrati per generare testo.

**Grammatica**: Sistema di regole che governano la struttura di una lingua.

## H

**Hidden Markov Model (HMM)**: Modello statistico in cui il sistema è modellato come un processo markoviano con stati non osservabili.

**Hyperparameter**: Parametro del modello il cui valore viene impostato prima dell'inizio del processo di apprendimento.

## I

**Information Retrieval**: Campo che si occupa di ottenere informazioni rilevanti da una collezione di risorse.

**Intent Recognition**: Processo di identificazione dell'intenzione dell'utente in un sistema di dialogo.

## K

**K-means**: Algoritmo di clustering che divide i dati in k gruppi basandosi sulla distanza dai centroidi.

**Knowledge Graph**: Rappresentazione strutturata della conoscenza che collega entità e concetti.

## L

**Lemmatizzazione**: Processo di riduzione di una parola alla sua forma base (lemma), considerando il contesto e l'analisi morfologica.

**LSTM (Long Short-Term Memory)**: Tipo di rete neurale ricorrente progettata per riconoscere pattern in sequenze di dati.

## M

**Matrice di confusione**: Tabella utilizzata per valutare le prestazioni di un algoritmo di classificazione.

**Modello linguistico (Language Model)**: Sistema che assegna probabilità a sequenze di parole.

**Morfologia**: Studio della struttura e della formazione delle parole.

## N

**N-gram**: Sequenza contigua di n elementi da un testo.

**NER (Named Entity Recognition)**: Processo di identificazione e classificazione di entità nominali in un testo.

**Normalizzazione del testo**: Processo di trasformazione del testo in un formato standard per facilitarne l'analisi.

**NLU (Natural Language Understanding)**: Sottocampo dell'NLP che si concentra sulla comprensione del significato del linguaggio.

## O

**One-hot encoding**: Rappresentazione di variabili categoriche come vettori binari.

**Overfitting**: Fenomeno in cui un modello apprende troppo dai dati di addestramento, perdendo capacità di generalizzazione.

## P

**Part-of-Speech (POS) Tagging**: Processo di assegnazione di categorie grammaticali alle parole in un testo.

**Perplexity**: Misura di quanto bene un modello probabilistico predice un campione, utilizzata per valutare modelli linguistici.

**Pipeline NLP**: Sequenza di operazioni di elaborazione del testo applicate in successione.

**Precision**: Frazione di istanze rilevanti tra quelle recuperate.

**Preprocessing**: Fase di pulizia e preparazione dei dati prima dell'analisi.

## Q

**Query Expansion**: Processo di riformulazione di una query iniziale per migliorare le prestazioni di recupero.

## R

**Recall**: Frazione di istanze rilevanti che sono state recuperate.

**Rete neurale**: Modello computazionale ispirato al funzionamento del cervello umano.

**RNN (Recurrent Neural Network)**: Tipo di rete neurale progettata per riconoscere pattern in dati sequenziali.

## S

**Semantica**: Studio del significato nel linguaggio.

**Sentiment Analysis**: Vedi "Analisi del sentiment".

**Sequence-to-Sequence (Seq2Seq)**: Architettura di rete neurale utilizzata per convertire sequenze da un dominio a un altro.

**Sintassi**: Studio delle regole e dei principi che governano la struttura delle frasi.

**Stemming**: Processo di riduzione di una parola alla sua radice o tema, rimuovendo suffissi e prefissi.

**Stop words**: Parole comuni (come articoli, preposizioni) spesso rimosse durante il preprocessing del testo.

## T

**TF-IDF (Term Frequency-Inverse Document Frequency)**: Statistica numerica che riflette l'importanza di una parola in un documento rispetto a una collezione.

**Tokenizzazione**: Processo di suddivisione del testo in unità più piccole (token), come parole o frasi.

**Topic Modeling**: Tecnica statistica per scoprire gli "argomenti" astratti in una collezione di documenti.

**Transfer Learning**: Approccio in cui un modello sviluppato per un compito viene riutilizzato come punto di partenza per un altro compito.

**Transformer**: Architettura di rete neurale basata sul meccanismo di attenzione, fondamentale per molti modelli NLP moderni.

## U

**Underfitting**: Fenomeno in cui un modello è troppo semplice per catturare la complessità dei dati.

## V

**Vettorizzazione**: Processo di conversione del testo in rappresentazioni numeriche.

**Vocabolario**: Insieme di parole o token conosciuti da un modello NLP.

## W

**Word Embedding**: Rappresentazione di parole come vettori numerici in uno spazio multidimensionale.

**Word2Vec**: Tecnica per apprendere rappresentazioni vettoriali di parole basate sul contesto.

## X

**XLNet**: Modello di linguaggio pre-addestrato che generalizza l'approccio autoregressive di modelli come GPT.

## Z

**Zero-shot Learning**: Capacità di un modello di riconoscere oggetti o eseguire compiti che non ha mai visto durante l'addestramento.
