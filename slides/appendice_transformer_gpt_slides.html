<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,height=device-height,initial-scale=1.0"><meta name="apple-mobile-web-app-capable" content="yes"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta property="og:type" content="website"><meta name="twitter:card" content="summary"><style>@media screen{body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button{appearance:none;background-color:initial;border:0;color:inherit;cursor:pointer;font-size:inherit;opacity:.8;outline:none;padding:0;transition:opacity .2s linear;-webkit-tap-highlight-color:transparent}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button:disabled,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button:disabled,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button:disabled,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button:disabled{cursor:not-allowed;opacity:.15!important}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button:hover,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button:hover,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button:hover,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button:hover{opacity:1}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button:hover:active,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button:hover:active,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button:hover:active,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button:hover:active{opacity:.6}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button:hover:not(:disabled),body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button:hover:not(:disabled),body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button:hover:not(:disabled),body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button:hover:not(:disabled){transition:none}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=prev],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=prev],body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button.bespoke-marp-presenter-info-page-prev{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiNmZmYiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSI1IiBkPSJNNjggOTAgMjggNTBsNDAtNDAiLz48L3N2Zz4=") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=next],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=next],body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button.bespoke-marp-presenter-info-page-next{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiNmZmYiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSI1IiBkPSJtMzIgOTAgNDAtNDAtNDAtNDAiLz48L3N2Zz4=") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=fullscreen],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=fullscreen]{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48ZGVmcz48c3R5bGU+LmF7ZmlsbDpub25lO3N0cm9rZTojZmZmO3N0cm9rZS1saW5lY2FwOnJvdW5kO3N0cm9rZS1saW5lam9pbjpyb3VuZDtzdHJva2Utd2lkdGg6NXB4fTwvc3R5bGU+PC9kZWZzPjxyZWN0IHdpZHRoPSI4MCIgaGVpZ2h0PSI2MCIgeD0iMTAiIHk9IjIwIiBjbGFzcz0iYSIgcng9IjUuNjciLz48cGF0aCBkPSJNNDAgNzBIMjBWNTBtMjAgMEwyMCA3MG00MC00MGgyMHYyMG0tMjAgMCAyMC0yMCIgY2xhc3M9ImEiLz48L3N2Zz4=") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button.exit[data-bespoke-marp-osc=fullscreen],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button.exit[data-bespoke-marp-osc=fullscreen]{background-image:url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48ZGVmcz48c3R5bGU+LmF7ZmlsbDpub25lO3N0cm9rZTojZmZmO3N0cm9rZS1saW5lY2FwOnJvdW5kO3N0cm9rZS1saW5lam9pbjpyb3VuZDtzdHJva2Utd2lkdGg6NXB4fTwvc3R5bGU+PC9kZWZzPjxyZWN0IHdpZHRoPSI4MCIgaGVpZ2h0PSI2MCIgeD0iMTAiIHk9IjIwIiBjbGFzcz0iYSIgcng9IjUuNjciLz48cGF0aCBkPSJNMjAgNTBoMjB2MjBtLTIwIDAgMjAtMjBtNDAgMEg2MFYzMG0yMCAwTDYwIDUwIiBjbGFzcz0iYSIvPjwvc3ZnPg==")}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=presenter],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=presenter]{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiNmZmYiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSI1IiBkPSJNODcuOCA0Ny41Qzg5IDUwIDg3LjcgNTIgODUgNTJIMzVhOC43IDguNyAwIDAgMS03LjItNC41bC0xNS42LTMxQzExIDE0IDEyLjIgMTIgMTUgMTJoNTBhOC44IDguOCAwIDAgMSA3LjIgNC41ek02MCA1MnYzNm0tMTAgMGgyME00NSA0MmgyMCIvPjwvc3ZnPg==") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button.bespoke-marp-presenter-note-bigger{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48cGF0aCBzdHJva2U9IiNmZmYiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSI1IiBkPSJNMTIgNTBoODBNNTIgOTBWMTAiLz48L3N2Zz4=") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button.bespoke-marp-presenter-note-smaller{background:#0000 url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiNmZmYiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSI1IiBkPSJNMTIgNTBoODAiLz48L3N2Zz4=") no-repeat 50%;background-size:contain;overflow:hidden;text-indent:100%;white-space:nowrap}}@keyframes __bespoke_marp_transition_reduced_outgoing__{0%{opacity:1}to{opacity:0}}@keyframes __bespoke_marp_transition_reduced_incoming__{0%{mix-blend-mode:plus-lighter;opacity:0}to{mix-blend-mode:plus-lighter;opacity:1}}.bespoke-marp-note,.bespoke-marp-osc,.bespoke-progress-parent{display:none;transition:none}@media screen{::view-transition-group(*){animation-duration:var(--marp-bespoke-transition-animation-duration,.5s);animation-timing-function:ease}::view-transition-new(*),::view-transition-old(*){animation-delay:0s;animation-direction:var(--marp-bespoke-transition-animation-direction,normal);animation-duration:var(--marp-bespoke-transition-animation-duration,.5s);animation-fill-mode:both;animation-name:var(--marp-bespoke-transition-animation-name,var(--marp-bespoke-transition-animation-name-fallback,__bespoke_marp_transition_no_animation__));mix-blend-mode:normal}::view-transition-old(*){--marp-bespoke-transition-animation-name-fallback:__bespoke_marp_transition_reduced_outgoing__;animation-timing-function:ease}::view-transition-new(*){--marp-bespoke-transition-animation-name-fallback:__bespoke_marp_transition_reduced_incoming__;animation-timing-function:ease}::view-transition-new(root),::view-transition-old(root){animation-timing-function:linear}::view-transition-new(__bespoke_marp_transition_osc__),::view-transition-old(__bespoke_marp_transition_osc__){animation-duration:0s!important;animation-name:__bespoke_marp_transition_osc__!important}::view-transition-new(__bespoke_marp_transition_osc__){opacity:0!important}.bespoke-marp-transition-warming-up::view-transition-group(*),.bespoke-marp-transition-warming-up::view-transition-new(*),.bespoke-marp-transition-warming-up::view-transition-old(*){animation-play-state:paused!important}body,html{height:100%;margin:0}body{background:#000;overflow:hidden}svg.bespoke-marp-slide{content-visibility:hidden;opacity:0;pointer-events:none;z-index:-1}svg.bespoke-marp-slide:not(.bespoke-marp-active) *{view-transition-name:none!important}svg.bespoke-marp-slide.bespoke-marp-active{content-visibility:visible;opacity:1;pointer-events:auto;z-index:0}svg.bespoke-marp-slide.bespoke-marp-active.bespoke-marp-active-ready *{animation-name:__bespoke_marp__!important}@supports not (content-visibility:hidden){svg.bespoke-marp-slide[data-bespoke-marp-load=hideable]{display:none}svg.bespoke-marp-slide[data-bespoke-marp-load=hideable].bespoke-marp-active{display:block}}}@media screen and (prefers-reduced-motion:reduce){svg.bespoke-marp-slide *{view-transition-name:none!important}}@media screen{[data-bespoke-marp-fragment=inactive]{visibility:hidden}body[data-bespoke-view=""] .bespoke-marp-parent,body[data-bespoke-view=next] .bespoke-marp-parent{inset:0;position:absolute}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc{background:#000000a6;border-radius:7px;bottom:50px;color:#fff;contain:paint;display:block;font-family:Helvetica,Arial,sans-serif;font-size:16px;left:50%;line-height:0;opacity:1;padding:12px;position:absolute;touch-action:manipulation;transform:translateX(-50%);transition:opacity .2s linear;-webkit-user-select:none;user-select:none;white-space:nowrap;will-change:transform;z-index:1;view-transition-name:__bespoke_marp_transition_osc__}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>*,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>*{margin-left:6px}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>:first-child,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>:first-child{margin-left:0}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>span,body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>span{opacity:.8}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>span[data-bespoke-marp-osc=page],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>span[data-bespoke-marp-osc=page]{display:inline-block;min-width:140px;text-align:center}body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=fullscreen],body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=next],body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=presenter],body[data-bespoke-view=""] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=prev],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=fullscreen],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=next],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=presenter],body[data-bespoke-view=next] .bespoke-marp-parent>.bespoke-marp-osc>button[data-bespoke-marp-osc=prev]{height:32px;line-height:32px;width:32px}body[data-bespoke-view=""] .bespoke-marp-parent.bespoke-marp-inactive,body[data-bespoke-view=next] .bespoke-marp-parent.bespoke-marp-inactive{cursor:none}body[data-bespoke-view=""] .bespoke-marp-parent.bespoke-marp-inactive>.bespoke-marp-osc,body[data-bespoke-view=next] .bespoke-marp-parent.bespoke-marp-inactive>.bespoke-marp-osc{opacity:0;pointer-events:none}body[data-bespoke-view=""] svg.bespoke-marp-slide,body[data-bespoke-view=next] svg.bespoke-marp-slide{height:100%;left:0;position:absolute;top:0;width:100%}body[data-bespoke-view=""] .bespoke-progress-parent{background:#222;display:flex;height:5px;width:100%}body[data-bespoke-view=""] .bespoke-progress-parent+.bespoke-marp-parent{top:5px}body[data-bespoke-view=""] .bespoke-progress-parent .bespoke-progress-bar{background:#0288d1;flex:0 0 0;transition:flex-basis .2s cubic-bezier(0,1,1,1)}body[data-bespoke-view=next]{background:#0000}body[data-bespoke-view=presenter]{background:#161616}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container{display:grid;font-family:Helvetica,Arial,sans-serif;grid-template:"current dragbar next" minmax(140px,1fr) "current dragbar note" 2fr "info    dragbar note" 3em;grid-template-columns:minmax(3px,var(--bespoke-marp-presenter-split-ratio,66%)) 0 minmax(3px,1fr);height:100%;left:0;position:absolute;top:0;width:100%}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-parent{grid-area:current;overflow:hidden;position:relative}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-parent svg.bespoke-marp-slide{height:calc(100% - 40px);left:20px;pointer-events:none;position:absolute;top:20px;-webkit-user-select:none;user-select:none;width:calc(100% - 40px)}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-parent svg.bespoke-marp-slide.bespoke-marp-active{filter:drop-shadow(0 3px 10px rgba(0,0,0,.5))}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-dragbar-container{background:#0288d1;cursor:col-resize;grid-area:dragbar;margin-left:-3px;opacity:0;position:relative;transition:opacity .4s linear .1s;width:6px;z-index:10}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-dragbar-container:hover{opacity:1}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-dragbar-container.active{opacity:1;transition-delay:0s}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-next-container{background:#222;cursor:pointer;display:none;grid-area:next;overflow:hidden;position:relative}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-next-container.active{display:block}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-next-container iframe.bespoke-marp-presenter-next{background:#0000;border:0;display:block;filter:drop-shadow(0 3px 10px rgba(0,0,0,.5));height:calc(100% - 40px);left:20px;pointer-events:none;position:absolute;top:20px;-webkit-user-select:none;user-select:none;width:calc(100% - 40px)}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container{background:#222;color:#eee;grid-area:note;position:relative;z-index:1}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container button{height:1.5em;line-height:1.5em;width:1.5em}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-presenter-note-wrapper{display:block;inset:0;position:absolute}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-presenter-note-buttons{background:#000000a6;border-radius:4px;bottom:0;display:flex;gap:4px;margin:12px;opacity:0;padding:6px;pointer-events:none;position:absolute;right:0;transition:opacity .2s linear}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-presenter-note-buttons:focus-within,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-presenter-note-wrapper:focus-within+.bespoke-marp-presenter-note-buttons,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container:hover .bespoke-marp-presenter-note-buttons{opacity:1;pointer-events:auto}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note{box-sizing:border-box;font-size:calc(1.1em*var(--bespoke-marp-note-font-scale, 1));height:calc(100% - 40px);margin:20px;overflow:auto;padding-right:3px;white-space:pre-wrap;width:calc(100% - 40px);word-wrap:break-word;scrollbar-color:#eeeeee80 #0000;scrollbar-width:thin}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note::-webkit-scrollbar{width:6px}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note::-webkit-scrollbar-track{background:#0000}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note::-webkit-scrollbar-thumb{background:#eeeeee80;border-radius:6px}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note:empty{pointer-events:none}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note.active{display:block}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note p:first-child{margin-top:0}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-note-container .bespoke-marp-note p:last-child{margin-bottom:0}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container{align-items:center;box-sizing:border-box;color:#eee;display:flex;flex-wrap:nowrap;grid-area:info;justify-content:center;overflow:hidden;padding:0 10px}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-page,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-time,body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-timer{box-sizing:border-box;display:block;padding:0 10px;white-space:nowrap;width:100%}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container button{height:1.5em;line-height:1.5em;width:1.5em}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-page{order:2;text-align:center}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-page .bespoke-marp-presenter-info-page-text{display:inline-block;min-width:120px;text-align:center}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-time{color:#999;order:1;text-align:left}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-timer{color:#999;order:3;text-align:right}body[data-bespoke-view=presenter] .bespoke-marp-presenter-container .bespoke-marp-presenter-info-container .bespoke-marp-presenter-info-timer:hover{cursor:pointer}}@media print{.bespoke-marp-presenter-info-container,.bespoke-marp-presenter-next-container,.bespoke-marp-presenter-note-container{display:none}}</style><style>div#\:\$p > svg > foreignObject > section{width:1280px;height:720px;box-sizing:border-box;overflow:hidden;position:relative;scroll-snap-align:center center;-webkit-text-size-adjust:100%;text-size-adjust:100%}div#\:\$p > svg > foreignObject > section::after{bottom:0;content:attr(data-marpit-pagination);padding:inherit;pointer-events:none;position:absolute;right:0}div#\:\$p > svg > foreignObject > section:not([data-marpit-pagination])::after{display:none}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1){font-size:2em;margin:0.67em 0}div#\:\$p > svg > foreignObject > section video::-webkit-media-controls{will-change:transform}@page {size:1280px 720px;margin:0}@media print{html, body{background-color:#fff;margin:0;page-break-inside:avoid;break-inside:avoid-page}div#\:\$p > svg > foreignObject > section{page-break-before:always;break-before:page}div#\:\$p > svg > foreignObject > section, div#\:\$p > svg > foreignObject > section *{-webkit-print-color-adjust:exact!important;animation-delay:0s!important;animation-duration:0s!important;color-adjust:exact!important;transition:none!important}div#\:\$p > svg[data-marpit-svg]{display:block;height:100vh;width:100vw}}div#\:\$p > svg > foreignObject > :where(section){container-type:size}div#\:\$p > svg > foreignObject > section img[data-marp-twemoji]{background:transparent;height:1em;margin:0 .05em 0 .1em;vertical-align:-.1em;width:1em}/*!
 * Marp default theme.
 *
 * @theme default
 * @author Yuki Hattori
 *
 * @auto-scaling true
 * @size 16:9 1280px 720px
 * @size 4:3 960px 720px
 */div#\:\$p > svg > foreignObject > section{--base-size-4:calc(var(--marpit-root-font-size, 1rem) * 0.25);--base-size-8:calc(var(--marpit-root-font-size, 1rem) * 0.5);--base-size-16:calc(var(--marpit-root-font-size, 1rem) * 1);--base-size-24:calc(var(--marpit-root-font-size, 1rem) * 1.5);--base-size-40:calc(var(--marpit-root-font-size, 1rem) * 2.5);--base-text-weight-normal:400;--base-text-weight-medium:500;--base-text-weight-semibold:600;--fontStack-monospace:ui-monospace, SFMono-Regular, SF Mono, Menlo, Consolas, Liberation Mono, monospace;--fgColor-accent:Highlight;}div#\:\$p > svg > foreignObject > section [data-theme=light],div#\:\$p > svg > foreignObject > section{color-scheme:light;--focus-outlineColor:#0969da;--fgColor-default:#1f2328;--fgColor-muted:#59636e;--fgColor-accent:#0969da;--fgColor-success:#1a7f37;--fgColor-attention:#9a6700;--fgColor-danger:#d1242f;--fgColor-done:#8250df;--bgColor-default:#fff;--bgColor-muted:#f6f8fa;--bgColor-neutral-muted:#818b981f;--bgColor-attention-muted:#fff8c5;--borderColor-default:#d1d9e0;--borderColor-muted:#d1d9e0b3;--borderColor-neutral-muted:#d1d9e0b3;--borderColor-accent-emphasis:#0969da;--borderColor-success-emphasis:#1a7f37;--borderColor-attention-emphasis:#9a6700;--borderColor-danger-emphasis:#cf222e;--borderColor-done-emphasis:#8250df;--color-prettylights-syntax-comment:#59636e;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-prettylights-syntax-entity:#6639ba;--color-prettylights-syntax-storage-modifier-import:#1f2328;--color-prettylights-syntax-entity-tag:#0550ae;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-brackethighlighter-unmatched:#82071e;--color-prettylights-syntax-brackethighlighter-angle:#59636e;--color-prettylights-syntax-invalid-illegal-text:#f6f8fa;--color-prettylights-syntax-invalid-illegal-bg:#82071e;--color-prettylights-syntax-carriage-return-text:#f6f8fa;--color-prettylights-syntax-carriage-return-bg:#cf222e;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-markup-list:#3b2300;--color-prettylights-syntax-markup-heading:#0550ae;--color-prettylights-syntax-markup-italic:#1f2328;--color-prettylights-syntax-markup-bold:#1f2328;--color-prettylights-syntax-markup-deleted-text:#82071e;--color-prettylights-syntax-markup-deleted-bg:#ffebe9;--color-prettylights-syntax-markup-inserted-text:#116329;--color-prettylights-syntax-markup-inserted-bg:#dafbe1;--color-prettylights-syntax-markup-changed-text:#953800;--color-prettylights-syntax-markup-changed-bg:#ffd8b5;--color-prettylights-syntax-markup-ignored-text:#d1d9e0;--color-prettylights-syntax-markup-ignored-bg:#0550ae;--color-prettylights-syntax-meta-diff-range:#8250df;--color-prettylights-syntax-sublimelinter-gutter-mark:#818b98;}div#\:\$p > svg > foreignObject > section [data-theme=dark],div#\:\$p > svg > foreignObject > section:where(.invert){color-scheme:dark;--focus-outlineColor:#1f6feb;--fgColor-default:#f0f6fc;--fgColor-muted:#9198a1;--fgColor-accent:#4493f8;--fgColor-success:#3fb950;--fgColor-attention:#d29922;--fgColor-danger:#f85149;--fgColor-done:#ab7df8;--bgColor-default:#0d1117;--bgColor-muted:#151b23;--bgColor-neutral-muted:#656c7633;--bgColor-attention-muted:#bb800926;--borderColor-default:#3d444d;--borderColor-muted:#3d444db3;--borderColor-neutral-muted:#3d444db3;--borderColor-accent-emphasis:#1f6feb;--borderColor-success-emphasis:#238636;--borderColor-attention-emphasis:#9e6a03;--borderColor-danger-emphasis:#da3633;--borderColor-done-emphasis:#8957e5;--color-prettylights-syntax-comment:#9198a1;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#f0f6fc;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-brackethighlighter-unmatched:#f85149;--color-prettylights-syntax-brackethighlighter-angle:#9198a1;--color-prettylights-syntax-invalid-illegal-text:#f0f6fc;--color-prettylights-syntax-invalid-illegal-bg:#8e1519;--color-prettylights-syntax-carriage-return-text:#f0f6fc;--color-prettylights-syntax-carriage-return-bg:#b62324;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-markup-list:#f2cc60;--color-prettylights-syntax-markup-heading:#1f6feb;--color-prettylights-syntax-markup-italic:#f0f6fc;--color-prettylights-syntax-markup-bold:#f0f6fc;--color-prettylights-syntax-markup-deleted-text:#ffdcd7;--color-prettylights-syntax-markup-deleted-bg:#67060c;--color-prettylights-syntax-markup-inserted-text:#aff5b4;--color-prettylights-syntax-markup-inserted-bg:#033a16;--color-prettylights-syntax-markup-changed-text:#ffdfb6;--color-prettylights-syntax-markup-changed-bg:#5a1e02;--color-prettylights-syntax-markup-ignored-text:#f0f6fc;--color-prettylights-syntax-markup-ignored-bg:#1158c7;--color-prettylights-syntax-meta-diff-range:#d2a8ff;--color-prettylights-syntax-sublimelinter-gutter-mark:#3d444d;}div#\:\$p > svg > foreignObject > section{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;background-color:var(--bgColor-default);color:var(--fgColor-default);font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Noto Sans,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:16px;line-height:1.5;margin:0;word-wrap:break-word}div#\:\$p > svg > foreignObject > section{--marpit-root-font-size:16px;}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1):hover .anchor .octicon-link:before,div#\:\$p > svg > foreignObject > section :is(h2, marp-h2):hover .anchor .octicon-link:before,div#\:\$p > svg > foreignObject > section :is(h3, marp-h3):hover .anchor .octicon-link:before,div#\:\$p > svg > foreignObject > section :is(h4, marp-h4):hover .anchor .octicon-link:before,div#\:\$p > svg > foreignObject > section :is(h5, marp-h5):hover .anchor .octicon-link:before,div#\:\$p > svg > foreignObject > section :is(h6, marp-h6):hover .anchor .octicon-link:before{background-color:currentColor;content:" ";display:inline-block;height:16px;-webkit-mask-image:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 0 0 1.06 1.06l1.25-1.25a2 2 0 1 1 2.83 2.83l-2.5 2.5a2 2 0 0 1-2.83 0 .75.75 0 0 0-1.06 1.06 3.5 3.5 0 0 0 4.95 0l2.5-2.5a3.5 3.5 0 0 0-4.95-4.95zm-4.69 9.64a2 2 0 0 1 0-2.83l2.5-2.5a2 2 0 0 1 2.83 0 .75.75 0 0 0 1.06-1.06 3.5 3.5 0 0 0-4.95 0l-2.5 2.5a3.5 3.5 0 0 0 4.95 4.95l1.25-1.25a.75.75 0 0 0-1.06-1.06l-1.25 1.25a2 2 0 0 1-2.83 0"/></svg>');mask-image:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 0 0 1.06 1.06l1.25-1.25a2 2 0 1 1 2.83 2.83l-2.5 2.5a2 2 0 0 1-2.83 0 .75.75 0 0 0-1.06 1.06 3.5 3.5 0 0 0 4.95 0l2.5-2.5a3.5 3.5 0 0 0-4.95-4.95zm-4.69 9.64a2 2 0 0 1 0-2.83l2.5-2.5a2 2 0 0 1 2.83 0 .75.75 0 0 0 1.06-1.06 3.5 3.5 0 0 0-4.95 0l-2.5 2.5a3.5 3.5 0 0 0 4.95 4.95l1.25-1.25a.75.75 0 0 0-1.06-1.06l-1.25 1.25a2 2 0 0 1-2.83 0"/></svg>');width:16px}div#\:\$p > svg > foreignObject > section details,div#\:\$p > svg > foreignObject > section figcaption,div#\:\$p > svg > foreignObject > section figure{display:block}div#\:\$p > svg > foreignObject > section summary{display:list-item}div#\:\$p > svg > foreignObject > section [hidden]{display:none!important}div#\:\$p > svg > foreignObject > section a{background-color:transparent;color:var(--fgColor-accent);text-decoration:none}div#\:\$p > svg > foreignObject > section abbr[title]{border-bottom:none;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}div#\:\$p > svg > foreignObject > section b,div#\:\$p > svg > foreignObject > section strong{font-weight:var(--base-text-weight-semibold, 600)}div#\:\$p > svg > foreignObject > section dfn{font-style:italic}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1){border-bottom:1px solid var(--borderColor-muted);font-size:2em;font-weight:var(--base-text-weight-semibold, 600);margin:.67em 0;padding-bottom:.3em}div#\:\$p > svg > foreignObject > section mark{background-color:var(--bgColor-attention-muted);color:var(--fgColor-default)}div#\:\$p > svg > foreignObject > section small{font-size:90%}div#\:\$p > svg > foreignObject > section sub,div#\:\$p > svg > foreignObject > section sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}div#\:\$p > svg > foreignObject > section sub{bottom:-.25em}div#\:\$p > svg > foreignObject > section sup{top:-.5em}div#\:\$p > svg > foreignObject > section img{border-style:none;box-sizing:content-box;max-width:100%}div#\:\$p > svg > foreignObject > section code,div#\:\$p > svg > foreignObject > section kbd,div#\:\$p > svg > foreignObject > section :is(pre, marp-pre),div#\:\$p > svg > foreignObject > section samp{font-family:monospace;font-size:1em}div#\:\$p > svg > foreignObject > section figure{margin:1em var(--base-size-40)}div#\:\$p > svg > foreignObject > section hr{background:transparent;background-color:var(--borderColor-default);border:0;box-sizing:content-box;height:.25em;margin:var(--base-size-24) 0;overflow:hidden;padding:0}div#\:\$p > svg > foreignObject > section input{font:inherit;font-family:inherit;font-size:inherit;line-height:inherit;margin:0;overflow:visible}div#\:\$p > svg > foreignObject > section [type=button],div#\:\$p > svg > foreignObject > section [type=reset],div#\:\$p > svg > foreignObject > section [type=submit]{-webkit-appearance:button;-moz-appearance:button;appearance:button}div#\:\$p > svg > foreignObject > section [type=checkbox],div#\:\$p > svg > foreignObject > section [type=radio]{box-sizing:border-box;padding:0}div#\:\$p > svg > foreignObject > section [type=number]::-webkit-inner-spin-button,div#\:\$p > svg > foreignObject > section [type=number]::-webkit-outer-spin-button{height:auto}div#\:\$p > svg > foreignObject > section [type=search]::-webkit-search-cancel-button,div#\:\$p > svg > foreignObject > section [type=search]::-webkit-search-decoration{-webkit-appearance:none;appearance:none}div#\:\$p > svg > foreignObject > section ::-webkit-input-placeholder{color:inherit;opacity:.54}div#\:\$p > svg > foreignObject > section ::-webkit-file-upload-button{-webkit-appearance:button;appearance:button;font:inherit}div#\:\$p > svg > foreignObject > section a:hover{text-decoration:underline}div#\:\$p > svg > foreignObject > section ::-moz-placeholder{color:var(--fgColor-muted);opacity:1}div#\:\$p > svg > foreignObject > section ::placeholder{color:var(--fgColor-muted);opacity:1}div#\:\$p > svg > foreignObject > section hr:after,div#\:\$p > svg > foreignObject > section hr:before{content:"";display:table}div#\:\$p > svg > foreignObject > section hr:after{clear:both}div#\:\$p > svg > foreignObject > section table{border-collapse:collapse;border-spacing:0;display:block;font-variant:tabular-nums;max-width:100%;overflow:auto;width:-moz-max-content;width:max-content}div#\:\$p > svg > foreignObject > section td,div#\:\$p > svg > foreignObject > section th{padding:0}div#\:\$p > svg > foreignObject > section details summary{cursor:pointer}div#\:\$p > svg > foreignObject > section [role=button]:focus,div#\:\$p > svg > foreignObject > section a:focus,div#\:\$p > svg > foreignObject > section input[type=checkbox]:focus,div#\:\$p > svg > foreignObject > section input[type=radio]:focus{box-shadow:none;outline:2px solid var(--focus-outlineColor);outline-offset:-2px}div#\:\$p > svg > foreignObject > section [role=button]:focus:not(:focus-visible),div#\:\$p > svg > foreignObject > section a:focus:not(:focus-visible),div#\:\$p > svg > foreignObject > section input[type=checkbox]:focus:not(:focus-visible),div#\:\$p > svg > foreignObject > section input[type=radio]:focus:not(:focus-visible){outline:1px solid transparent}div#\:\$p > svg > foreignObject > section [role=button]:focus-visible,div#\:\$p > svg > foreignObject > section a:focus-visible,div#\:\$p > svg > foreignObject > section input[type=checkbox]:focus-visible,div#\:\$p > svg > foreignObject > section input[type=radio]:focus-visible{box-shadow:none;outline:2px solid var(--focus-outlineColor);outline-offset:-2px}div#\:\$p > svg > foreignObject > section a:not([class]):focus,div#\:\$p > svg > foreignObject > section a:not([class]):focus-visible,div#\:\$p > svg > foreignObject > section input[type=checkbox]:focus,div#\:\$p > svg > foreignObject > section input[type=checkbox]:focus-visible,div#\:\$p > svg > foreignObject > section input[type=radio]:focus,div#\:\$p > svg > foreignObject > section input[type=radio]:focus-visible{outline-offset:0}div#\:\$p > svg > foreignObject > section kbd{background-color:var(--bgColor-muted);border-bottom-color:var(--borderColor-neutral-muted);border:1px solid var(--borderColor-neutral-muted);border-radius:6px;box-shadow:inset 0 -1px 0 var(--borderColor-neutral-muted);color:var(--fgColor-default);display:inline-block;font:11px var(--fontStack-monospace, ui-monospace, SFMono-Regular, SF Mono, Menlo, Consolas, Liberation Mono, monospace);line-height:10px;padding:var(--base-size-4);vertical-align:middle}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1),div#\:\$p > svg > foreignObject > section :is(h2, marp-h2),div#\:\$p > svg > foreignObject > section :is(h3, marp-h3),div#\:\$p > svg > foreignObject > section :is(h4, marp-h4),div#\:\$p > svg > foreignObject > section :is(h5, marp-h5),div#\:\$p > svg > foreignObject > section :is(h6, marp-h6){font-weight:var(--base-text-weight-semibold, 600);line-height:1.25;margin-bottom:var(--base-size-16);margin-top:var(--base-size-24)}div#\:\$p > svg > foreignObject > section :is(h2, marp-h2){border-bottom:1px solid var(--borderColor-muted);font-size:1.5em;padding-bottom:.3em}div#\:\$p > svg > foreignObject > section :is(h2, marp-h2),div#\:\$p > svg > foreignObject > section :is(h3, marp-h3){font-weight:var(--base-text-weight-semibold, 600)}div#\:\$p > svg > foreignObject > section :is(h3, marp-h3){font-size:1.25em}div#\:\$p > svg > foreignObject > section :is(h4, marp-h4){font-size:1em}div#\:\$p > svg > foreignObject > section :is(h4, marp-h4),div#\:\$p > svg > foreignObject > section :is(h5, marp-h5){font-weight:var(--base-text-weight-semibold, 600)}div#\:\$p > svg > foreignObject > section :is(h5, marp-h5){font-size:.875em}div#\:\$p > svg > foreignObject > section :is(h6, marp-h6){color:var(--fgColor-muted);font-size:.85em;font-weight:var(--base-text-weight-semibold, 600)}div#\:\$p > svg > foreignObject > section p{margin-bottom:10px;margin-top:0}div#\:\$p > svg > foreignObject > section blockquote{border-left:.25em solid var(--borderColor-default);color:var(--fgColor-muted);margin:0;padding:0 1em}div#\:\$p > svg > foreignObject > section ol,div#\:\$p > svg > foreignObject > section ul{margin-bottom:0;margin-top:0;padding-left:2em}div#\:\$p > svg > foreignObject > section ol ol,div#\:\$p > svg > foreignObject > section ul ol{list-style-type:lower-roman}div#\:\$p > svg > foreignObject > section ol ol ol,div#\:\$p > svg > foreignObject > section ol ul ol,div#\:\$p > svg > foreignObject > section ul ol ol,div#\:\$p > svg > foreignObject > section ul ul ol{list-style-type:lower-alpha}div#\:\$p > svg > foreignObject > section dd{margin-left:0}div#\:\$p > svg > foreignObject > section code,div#\:\$p > svg > foreignObject > section :is(pre, marp-pre),div#\:\$p > svg > foreignObject > section samp,div#\:\$p > svg > foreignObject > section tt{font-family:var(--fontStack-monospace, ui-monospace, SFMono-Regular, SF Mono, Menlo, Consolas, Liberation Mono, monospace);font-size:12px}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre){margin-bottom:0;margin-top:0;word-wrap:normal}div#\:\$p > svg > foreignObject > section .octicon{display:inline-block;overflow:visible!important;vertical-align:text-bottom;fill:currentColor}div#\:\$p > svg > foreignObject > section input::-webkit-inner-spin-button,div#\:\$p > svg > foreignObject > section input::-webkit-outer-spin-button{-webkit-appearance:none;appearance:none;margin:0}div#\:\$p > svg > foreignObject > section .mr-2{margin-right:var(--base-size-8, 8px)!important}div#\:\$p > svg > foreignObject > section:after,div#\:\$p > svg > foreignObject > section:before{display:table}div#\:\$p > svg > foreignObject > section:after{clear:both}div#\:\$p > svg > foreignObject > section>:first-child{margin-top:0!important}div#\:\$p > svg > foreignObject > section>:last-child{margin-bottom:0!important}div#\:\$p > svg > foreignObject > section a:not([href]){color:inherit;text-decoration:none}div#\:\$p > svg > foreignObject > section .absent{color:var(--fgColor-danger)}div#\:\$p > svg > foreignObject > section .anchor{float:left;line-height:1;margin-left:-20px;padding-right:var(--base-size-4)}div#\:\$p > svg > foreignObject > section .anchor:focus{outline:none}div#\:\$p > svg > foreignObject > section blockquote,div#\:\$p > svg > foreignObject > section details,div#\:\$p > svg > foreignObject > section dl,div#\:\$p > svg > foreignObject > section ol,div#\:\$p > svg > foreignObject > section p,div#\:\$p > svg > foreignObject > section :is(pre, marp-pre),div#\:\$p > svg > foreignObject > section table,div#\:\$p > svg > foreignObject > section ul{margin-bottom:var(--base-size-16);margin-top:0}div#\:\$p > svg > foreignObject > section blockquote>:first-child{margin-top:0}div#\:\$p > svg > foreignObject > section blockquote>:last-child{margin-bottom:0}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1) .octicon-link,div#\:\$p > svg > foreignObject > section :is(h2, marp-h2) .octicon-link,div#\:\$p > svg > foreignObject > section :is(h3, marp-h3) .octicon-link,div#\:\$p > svg > foreignObject > section :is(h4, marp-h4) .octicon-link,div#\:\$p > svg > foreignObject > section :is(h5, marp-h5) .octicon-link,div#\:\$p > svg > foreignObject > section :is(h6, marp-h6) .octicon-link{color:var(--fgColor-default);vertical-align:middle;visibility:hidden}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1):hover .anchor,div#\:\$p > svg > foreignObject > section :is(h2, marp-h2):hover .anchor,div#\:\$p > svg > foreignObject > section :is(h3, marp-h3):hover .anchor,div#\:\$p > svg > foreignObject > section :is(h4, marp-h4):hover .anchor,div#\:\$p > svg > foreignObject > section :is(h5, marp-h5):hover .anchor,div#\:\$p > svg > foreignObject > section :is(h6, marp-h6):hover .anchor{text-decoration:none}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1):hover .anchor .octicon-link,div#\:\$p > svg > foreignObject > section :is(h2, marp-h2):hover .anchor .octicon-link,div#\:\$p > svg > foreignObject > section :is(h3, marp-h3):hover .anchor .octicon-link,div#\:\$p > svg > foreignObject > section :is(h4, marp-h4):hover .anchor .octicon-link,div#\:\$p > svg > foreignObject > section :is(h5, marp-h5):hover .anchor .octicon-link,div#\:\$p > svg > foreignObject > section :is(h6, marp-h6):hover .anchor .octicon-link{visibility:visible}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1) code,div#\:\$p > svg > foreignObject > section :is(h1, marp-h1) tt,div#\:\$p > svg > foreignObject > section :is(h2, marp-h2) code,div#\:\$p > svg > foreignObject > section :is(h2, marp-h2) tt,div#\:\$p > svg > foreignObject > section :is(h3, marp-h3) code,div#\:\$p > svg > foreignObject > section :is(h3, marp-h3) tt,div#\:\$p > svg > foreignObject > section :is(h4, marp-h4) code,div#\:\$p > svg > foreignObject > section :is(h4, marp-h4) tt,div#\:\$p > svg > foreignObject > section :is(h5, marp-h5) code,div#\:\$p > svg > foreignObject > section :is(h5, marp-h5) tt,div#\:\$p > svg > foreignObject > section :is(h6, marp-h6) code,div#\:\$p > svg > foreignObject > section :is(h6, marp-h6) tt{font-size:inherit;padding:0 .2em}div#\:\$p > svg > foreignObject > section summary :is(h1, marp-h1),div#\:\$p > svg > foreignObject > section summary :is(h2, marp-h2),div#\:\$p > svg > foreignObject > section summary :is(h3, marp-h3),div#\:\$p > svg > foreignObject > section summary :is(h4, marp-h4),div#\:\$p > svg > foreignObject > section summary :is(h5, marp-h5),div#\:\$p > svg > foreignObject > section summary :is(h6, marp-h6){display:inline-block}div#\:\$p > svg > foreignObject > section summary :is(h1, marp-h1) .anchor,div#\:\$p > svg > foreignObject > section summary :is(h2, marp-h2) .anchor,div#\:\$p > svg > foreignObject > section summary :is(h3, marp-h3) .anchor,div#\:\$p > svg > foreignObject > section summary :is(h4, marp-h4) .anchor,div#\:\$p > svg > foreignObject > section summary :is(h5, marp-h5) .anchor,div#\:\$p > svg > foreignObject > section summary :is(h6, marp-h6) .anchor{margin-left:-40px}div#\:\$p > svg > foreignObject > section summary :is(h1, marp-h1),div#\:\$p > svg > foreignObject > section summary :is(h2, marp-h2){border-bottom:0;padding-bottom:0}div#\:\$p > svg > foreignObject > section ol.no-list,div#\:\$p > svg > foreignObject > section ul.no-list{list-style-type:none;padding:0}div#\:\$p > svg > foreignObject > section ol[type="a s"]{list-style-type:lower-alpha}div#\:\$p > svg > foreignObject > section ol[type="A s"]{list-style-type:upper-alpha}div#\:\$p > svg > foreignObject > section ol[type="i s"]{list-style-type:lower-roman}div#\:\$p > svg > foreignObject > section ol[type="I s"]{list-style-type:upper-roman}div#\:\$p > svg > foreignObject > section div>ol:not([type]),div#\:\$p > svg > foreignObject > section ol[type="1"]{list-style-type:decimal}div#\:\$p > svg > foreignObject > section ol ol,div#\:\$p > svg > foreignObject > section ol ul,div#\:\$p > svg > foreignObject > section ul ol,div#\:\$p > svg > foreignObject > section ul ul{margin-bottom:0;margin-top:0}div#\:\$p > svg > foreignObject > section li>p{margin-top:var(--base-size-16)}div#\:\$p > svg > foreignObject > section li+li{margin-top:.25em}div#\:\$p > svg > foreignObject > section dl{padding:0}div#\:\$p > svg > foreignObject > section dl dt{font-size:1em;font-style:italic;font-weight:var(--base-text-weight-semibold, 600);margin-top:var(--base-size-16);padding:0}div#\:\$p > svg > foreignObject > section dl dd{margin-bottom:var(--base-size-16);padding:0 var(--base-size-16)}div#\:\$p > svg > foreignObject > section table th{font-weight:var(--base-text-weight-semibold, 600)}div#\:\$p > svg > foreignObject > section table td,div#\:\$p > svg > foreignObject > section table th{border:1px solid var(--borderColor-default);padding:6px 13px}div#\:\$p > svg > foreignObject > section table td>:last-child{margin-bottom:0}div#\:\$p > svg > foreignObject > section table tr{background-color:var(--bgColor-default);border-top:1px solid var(--borderColor-muted)}div#\:\$p > svg > foreignObject > section table tr:nth-child(2n){background-color:var(--bgColor-muted)}div#\:\$p > svg > foreignObject > section table img{background-color:transparent}div#\:\$p > svg > foreignObject > section img[align=right]{padding-left:20px}div#\:\$p > svg > foreignObject > section img[align=left]{padding-right:20px}div#\:\$p > svg > foreignObject > section .emoji{background-color:transparent;max-width:none;vertical-align:text-top}div#\:\$p > svg > foreignObject > section :is(span, marp-span).frame,div#\:\$p > svg > foreignObject > section :is(span, marp-span).frame>:is(span, marp-span){display:block;overflow:hidden}div#\:\$p > svg > foreignObject > section :is(span, marp-span).frame>:is(span, marp-span){border:1px solid var(--borderColor-default);float:left;margin:13px 0 0;padding:7px;width:auto}div#\:\$p > svg > foreignObject > section :is(span, marp-span).frame :is(span, marp-span) img{display:block;float:left}div#\:\$p > svg > foreignObject > section :is(span, marp-span).frame :is(span, marp-span) :is(span, marp-span){clear:both;color:var(--fgColor-default);display:block;padding:5px 0 0}div#\:\$p > svg > foreignObject > section :is(span, marp-span).align-center{clear:both;display:block;overflow:hidden}div#\:\$p > svg > foreignObject > section :is(span, marp-span).align-center>:is(span, marp-span){display:block;margin:13px auto 0;overflow:hidden;text-align:center}div#\:\$p > svg > foreignObject > section :is(span, marp-span).align-center :is(span, marp-span) img{margin:0 auto;text-align:center}div#\:\$p > svg > foreignObject > section :is(span, marp-span).align-right{clear:both;display:block;overflow:hidden}div#\:\$p > svg > foreignObject > section :is(span, marp-span).align-right>:is(span, marp-span){display:block;margin:13px 0 0;overflow:hidden;text-align:right}div#\:\$p > svg > foreignObject > section :is(span, marp-span).align-right :is(span, marp-span) img{margin:0;text-align:right}div#\:\$p > svg > foreignObject > section :is(span, marp-span).float-left{display:block;float:left;margin-right:13px;overflow:hidden}div#\:\$p > svg > foreignObject > section :is(span, marp-span).float-left :is(span, marp-span){margin:13px 0 0}div#\:\$p > svg > foreignObject > section :is(span, marp-span).float-right{display:block;float:right;margin-left:13px;overflow:hidden}div#\:\$p > svg > foreignObject > section :is(span, marp-span).float-right>:is(span, marp-span){display:block;margin:13px auto 0;overflow:hidden;text-align:right}div#\:\$p > svg > foreignObject > section code,div#\:\$p > svg > foreignObject > section tt{background-color:var(--bgColor-neutral-muted);border-radius:6px;font-size:85%;margin:0;padding:.2em .4em;white-space:break-spaces}div#\:\$p > svg > foreignObject > section code br,div#\:\$p > svg > foreignObject > section tt br{display:none}div#\:\$p > svg > foreignObject > section del code{text-decoration:inherit}div#\:\$p > svg > foreignObject > section samp{font-size:85%}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) code{font-size:100%}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre)>code{background:transparent;border:0;margin:0;padding:0;white-space:pre;word-break:normal}div#\:\$p > svg > foreignObject > section .highlight{margin-bottom:var(--base-size-16)}div#\:\$p > svg > foreignObject > section .highlight :is(pre, marp-pre){margin-bottom:0;word-break:normal}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre){background-color:var(--bgColor-muted);border-radius:6px;color:var(--fgColor-default);font-size:85%;line-height:1.45;overflow:auto;padding:var(--base-size-16)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) code,div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) tt{display:inline;line-height:inherit;margin:0;max-width:auto;overflow:visible;padding:0;word-wrap:normal;background-color:transparent;border:0}div#\:\$p > svg > foreignObject > section .csv-data td,div#\:\$p > svg > foreignObject > section .csv-data th{font-size:12px;line-height:1;overflow:hidden;padding:5px;text-align:left;white-space:nowrap}div#\:\$p > svg > foreignObject > section .csv-data .blob-num{background:var(--bgColor-default);border:0;padding:10px var(--base-size-8) 9px;text-align:right}div#\:\$p > svg > foreignObject > section .csv-data tr{border-top:0}div#\:\$p > svg > foreignObject > section .csv-data th{background:var(--bgColor-muted);border-top:0;font-weight:var(--base-text-weight-semibold, 600)}div#\:\$p > svg > foreignObject > section [data-footnote-ref]:before{content:"["}div#\:\$p > svg > foreignObject > section [data-footnote-ref]:after{content:"]"}div#\:\$p > svg > foreignObject > section .footnotes{border-top:1px solid var(--borderColor-default);color:var(--fgColor-muted);font-size:12px}div#\:\$p > svg > foreignObject > section div#\:\$p > svg > foreignObject > section section.footnotes{--marpit-root-font-size:12px;}div#\:\$p > svg > foreignObject > section .footnotes ol,div#\:\$p > svg > foreignObject > section .footnotes ol ul{padding-left:var(--base-size-16)}div#\:\$p > svg > foreignObject > section .footnotes ol ul{display:inline-block;margin-top:var(--base-size-16)}div#\:\$p > svg > foreignObject > section .footnotes li{position:relative}div#\:\$p > svg > foreignObject > section .footnotes li:target:before{border:2px solid var(--borderColor-accent-emphasis);border-radius:6px;bottom:calc(var(--base-size-8)*-1);content:"";left:calc(var(--base-size-24)*-1);pointer-events:none;position:absolute;right:calc(var(--base-size-8)*-1);top:calc(var(--base-size-8)*-1)}div#\:\$p > svg > foreignObject > section .footnotes li:target{color:var(--fgColor-default)}div#\:\$p > svg > foreignObject > section .footnotes .data-footnote-backref g-emoji{font-family:monospace}div#\:\$p > svg > foreignObject > section body:has(:modal){padding-right:var(--dialog-scrollgutter)!important}div#\:\$p > svg > foreignObject > section .pl-c{color:var(--color-prettylights-syntax-comment)}div#\:\$p > svg > foreignObject > section .pl-c1,div#\:\$p > svg > foreignObject > section .pl-s .pl-v{color:var(--color-prettylights-syntax-constant)}div#\:\$p > svg > foreignObject > section .pl-e,div#\:\$p > svg > foreignObject > section .pl-en{color:var(--color-prettylights-syntax-entity)}div#\:\$p > svg > foreignObject > section .pl-s .pl-s1,div#\:\$p > svg > foreignObject > section .pl-smi{color:var(--color-prettylights-syntax-storage-modifier-import)}div#\:\$p > svg > foreignObject > section .pl-ent{color:var(--color-prettylights-syntax-entity-tag)}div#\:\$p > svg > foreignObject > section .pl-k{color:var(--color-prettylights-syntax-keyword)}div#\:\$p > svg > foreignObject > section .pl-pds,div#\:\$p > svg > foreignObject > section .pl-s,div#\:\$p > svg > foreignObject > section .pl-s .pl-pse .pl-s1,div#\:\$p > svg > foreignObject > section .pl-sr,div#\:\$p > svg > foreignObject > section .pl-sr .pl-cce,div#\:\$p > svg > foreignObject > section .pl-sr .pl-sra,div#\:\$p > svg > foreignObject > section .pl-sr .pl-sre{color:var(--color-prettylights-syntax-string)}div#\:\$p > svg > foreignObject > section .pl-smw,div#\:\$p > svg > foreignObject > section .pl-v{color:var(--color-prettylights-syntax-variable)}div#\:\$p > svg > foreignObject > section .pl-bu{color:var(--color-prettylights-syntax-brackethighlighter-unmatched)}div#\:\$p > svg > foreignObject > section .pl-ii{background-color:var(--color-prettylights-syntax-invalid-illegal-bg);color:var(--color-prettylights-syntax-invalid-illegal-text)}div#\:\$p > svg > foreignObject > section .pl-c2{background-color:var(--color-prettylights-syntax-carriage-return-bg);color:var(--color-prettylights-syntax-carriage-return-text)}div#\:\$p > svg > foreignObject > section .pl-sr .pl-cce{color:var(--color-prettylights-syntax-string-regexp);font-weight:700}div#\:\$p > svg > foreignObject > section .pl-ml{color:var(--color-prettylights-syntax-markup-list)}div#\:\$p > svg > foreignObject > section .pl-mh,div#\:\$p > svg > foreignObject > section .pl-mh .pl-en,div#\:\$p > svg > foreignObject > section .pl-ms{color:var(--color-prettylights-syntax-markup-heading);font-weight:700}div#\:\$p > svg > foreignObject > section .pl-mi{color:var(--color-prettylights-syntax-markup-italic);font-style:italic}div#\:\$p > svg > foreignObject > section .pl-mb{color:var(--color-prettylights-syntax-markup-bold);font-weight:700}div#\:\$p > svg > foreignObject > section .pl-md{background-color:var(--color-prettylights-syntax-markup-deleted-bg);color:var(--color-prettylights-syntax-markup-deleted-text)}div#\:\$p > svg > foreignObject > section .pl-mi1{background-color:var(--color-prettylights-syntax-markup-inserted-bg);color:var(--color-prettylights-syntax-markup-inserted-text)}div#\:\$p > svg > foreignObject > section .pl-mc{background-color:var(--color-prettylights-syntax-markup-changed-bg);color:var(--color-prettylights-syntax-markup-changed-text)}div#\:\$p > svg > foreignObject > section .pl-mi2{background-color:var(--color-prettylights-syntax-markup-ignored-bg);color:var(--color-prettylights-syntax-markup-ignored-text)}div#\:\$p > svg > foreignObject > section .pl-mdr{color:var(--color-prettylights-syntax-meta-diff-range);font-weight:700}div#\:\$p > svg > foreignObject > section .pl-ba{color:var(--color-prettylights-syntax-brackethighlighter-angle)}div#\:\$p > svg > foreignObject > section .pl-sg{color:var(--color-prettylights-syntax-sublimelinter-gutter-mark)}div#\:\$p > svg > foreignObject > section .pl-corl{color:var(--color-prettylights-syntax-constant-other-reference-link);text-decoration:underline}div#\:\$p > svg > foreignObject > section [role=button]:focus:not(:focus-visible),div#\:\$p > svg > foreignObject > section [role=tabpanel][tabindex="0"]:focus:not(:focus-visible),div#\:\$p > svg > foreignObject > section a:focus:not(:focus-visible),div#\:\$p > svg > foreignObject > section button:focus:not(:focus-visible),div#\:\$p > svg > foreignObject > section summary:focus:not(:focus-visible){box-shadow:none;outline:none}div#\:\$p > svg > foreignObject > section [tabindex="0"]:focus:not(:focus-visible),div#\:\$p > svg > foreignObject > section details-dialog:focus:not(:focus-visible){outline:none}div#\:\$p > svg > foreignObject > section g-emoji{display:inline-block;font-family:Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol;font-size:1em;font-style:normal!important;font-weight:var(--base-text-weight-normal, 400);line-height:1;min-width:1ch;vertical-align:-.075em}div#\:\$p > svg > foreignObject > section g-emoji img{height:1em;width:1em}div#\:\$p > svg > foreignObject > section .task-list-item{list-style-type:none}div#\:\$p > svg > foreignObject > section .task-list-item label{font-weight:var(--base-text-weight-normal, 400)}div#\:\$p > svg > foreignObject > section .task-list-item.enabled label{cursor:pointer}div#\:\$p > svg > foreignObject > section .task-list-item+.task-list-item{margin-top:var(--base-size-4)}div#\:\$p > svg > foreignObject > section .task-list-item .handle{display:none}div#\:\$p > svg > foreignObject > section .task-list-item-checkbox{margin:0 .2em .25em -1.4em;vertical-align:middle}div#\:\$p > svg > foreignObject > section ul:dir(rtl) .task-list-item-checkbox{margin:0 -1.6em .25em .2em}div#\:\$p > svg > foreignObject > section ol:dir(rtl) .task-list-item-checkbox{margin:0 -1.6em .25em .2em}div#\:\$p > svg > foreignObject > section .contains-task-list:focus-within .task-list-item-convert-container,div#\:\$p > svg > foreignObject > section .contains-task-list:hover .task-list-item-convert-container{display:block;height:24px;overflow:visible;width:auto;clip:auto}div#\:\$p > svg > foreignObject > section ::-webkit-calendar-picker-indicator{filter:invert(50%)}div#\:\$p > svg > foreignObject > section .markdown-alert{border-left:.25em solid var(--borderColor-default);color:inherit;margin-bottom:var(--base-size-16);padding:var(--base-size-8) var(--base-size-16)}div#\:\$p > svg > foreignObject > section .markdown-alert>:first-child{margin-top:0}div#\:\$p > svg > foreignObject > section .markdown-alert>:last-child{margin-bottom:0}div#\:\$p > svg > foreignObject > section .markdown-alert .markdown-alert-title{align-items:center;display:flex;font-weight:var(--base-text-weight-medium, 500);line-height:1}div#\:\$p > svg > foreignObject > section .markdown-alert.markdown-alert-note{border-left-color:var(--borderColor-accent-emphasis)}div#\:\$p > svg > foreignObject > section .markdown-alert.markdown-alert-note .markdown-alert-title{color:var(--fgColor-accent)}div#\:\$p > svg > foreignObject > section .markdown-alert.markdown-alert-important{border-left-color:var(--borderColor-done-emphasis)}div#\:\$p > svg > foreignObject > section .markdown-alert.markdown-alert-important .markdown-alert-title{color:var(--fgColor-done)}div#\:\$p > svg > foreignObject > section .markdown-alert.markdown-alert-warning{border-left-color:var(--borderColor-attention-emphasis)}div#\:\$p > svg > foreignObject > section .markdown-alert.markdown-alert-warning .markdown-alert-title{color:var(--fgColor-attention)}div#\:\$p > svg > foreignObject > section .markdown-alert.markdown-alert-tip{border-left-color:var(--borderColor-success-emphasis)}div#\:\$p > svg > foreignObject > section .markdown-alert.markdown-alert-tip .markdown-alert-title{color:var(--fgColor-success)}div#\:\$p > svg > foreignObject > section .markdown-alert.markdown-alert-caution{border-left-color:var(--borderColor-danger-emphasis)}div#\:\$p > svg > foreignObject > section .markdown-alert.markdown-alert-caution .markdown-alert-title{color:var(--fgColor-danger)}div#\:\$p > svg > foreignObject > section>:first-child>.heading-element:first-child{margin-top:0!important}div#\:\$p > svg > foreignObject > section .highlight :is(pre, marp-pre):has(+.zeroclipboard-container){min-height:52px}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1){color:var(--h1-color);font-size:1.6em}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1),div#\:\$p > svg > foreignObject > section :is(h2, marp-h2){border-bottom:none}div#\:\$p > svg > foreignObject > section :is(h2, marp-h2){font-size:1.3em}div#\:\$p > svg > foreignObject > section :is(h3, marp-h3){font-size:1.1em}div#\:\$p > svg > foreignObject > section :is(h4, marp-h4){font-size:1.05em}div#\:\$p > svg > foreignObject > section :is(h5, marp-h5){font-size:1em}div#\:\$p > svg > foreignObject > section :is(h6, marp-h6){font-size:.9em}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1) strong,div#\:\$p > svg > foreignObject > section :is(h2, marp-h2) strong,div#\:\$p > svg > foreignObject > section :is(h3, marp-h3) strong,div#\:\$p > svg > foreignObject > section :is(h4, marp-h4) strong,div#\:\$p > svg > foreignObject > section :is(h5, marp-h5) strong,div#\:\$p > svg > foreignObject > section :is(h6, marp-h6) strong{color:var(--heading-strong-color);font-weight:inherit}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1)::part(auto-scaling),div#\:\$p > svg > foreignObject > section :is(h2, marp-h2)::part(auto-scaling),div#\:\$p > svg > foreignObject > section :is(h3, marp-h3)::part(auto-scaling),div#\:\$p > svg > foreignObject > section :is(h4, marp-h4)::part(auto-scaling),div#\:\$p > svg > foreignObject > section :is(h5, marp-h5)::part(auto-scaling),div#\:\$p > svg > foreignObject > section :is(h6, marp-h6)::part(auto-scaling){max-height:563px}div#\:\$p > svg > foreignObject > section hr{height:0;padding-top:.25em}div#\:\$p > svg > foreignObject > section img{background-color:transparent}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre){border:1px solid var(--borderColor-default);line-height:1.15;overflow:visible}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre)::part(auto-scaling){max-height:529px}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs){color:var(--color-prettylights-syntax-storage-modifier-import)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-doctag),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-keyword),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-meta .hljs-keyword),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-template-tag),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-template-variable),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-type),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-variable.language_){color:var(--color-prettylights-syntax-keyword)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-title),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-title.class_),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-title.class_.inherited__),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-title.function_){color:var(--color-prettylights-syntax-entity)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-attr),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-attribute),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-literal),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-meta),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-number),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-operator),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-selector-attr),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-selector-class),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-selector-id),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-variable){color:var(--color-prettylights-syntax-constant)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-meta .hljs-string),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-regexp),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-string){color:var(--color-prettylights-syntax-string)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-built_in),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-symbol){color:var(--color-prettylights-syntax-variable)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-code),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-comment),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-formula){color:var(--color-prettylights-syntax-comment)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-name),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-quote),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-selector-pseudo),div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-selector-tag){color:var(--color-prettylights-syntax-entity-tag)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-subst){color:var(--color-prettylights-syntax-storage-modifier-import)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-section){color:var(--color-prettylights-syntax-markup-heading);font-weight:700}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-bullet){color:var(--color-prettylights-syntax-markup-list)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-emphasis){color:var(--color-prettylights-syntax-markup-italic);font-style:italic}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-strong){color:var(--color-prettylights-syntax-markup-bold);font-weight:700}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-addition){background-color:var(--color-prettylights-syntax-markup-inserted-bg);color:var(--color-prettylights-syntax-markup-inserted-text)}div#\:\$p > svg > foreignObject > section :is(pre, marp-pre) :where(.hljs-deletion){background-color:var(--color-prettylights-syntax-markup-deleted-bg);color:var(--color-prettylights-syntax-markup-deleted-text)}div#\:\$p > svg > foreignObject > section footer,div#\:\$p > svg > foreignObject > section header{color:var(--header-footer-color);font-size:18px;left:30px;margin:0;position:absolute}div#\:\$p > svg > foreignObject > section header{top:21px}div#\:\$p > svg > foreignObject > section footer{bottom:21px}div#\:\$p > svg > foreignObject > section{--h1-color:#246;--header-footer-color:hsla(0,0%,40%,.75);--heading-strong-color:#48c;--paginate-color:#777;--base-size-4:4px;--base-size-8:8px;--base-size-16:16px;--base-size-24:24px;--base-size-40:40px;align-items:stretch;display:block;flex-flow:column nowrap;font-size:29px;height:720px;padding:78.5px;place-content:safe center center;width:1280px}div#\:\$p > svg > foreignObject > section{--marpit-root-font-size:29px;}div#\:\$p > svg > foreignObject > section:where(.invert){--h1-color:#cee7ff;--header-footer-color:hsla(0,0%,60%,.75);--heading-strong-color:#7bf;--paginate-color:#999;}div#\:\$p > svg > foreignObject > section>:last-child,div#\:\$p > svg > foreignObject > section[data-footer]>:nth-last-child(2){margin-bottom:0}div#\:\$p > svg > foreignObject > section>:first-child,div#\:\$p > svg > foreignObject > section>header:first-child+*{margin-top:0}div#\:\$p > svg > foreignObject > section:after{bottom:21px;color:var(--paginate-color);font-size:24px;padding:0;position:absolute;right:30px}div#\:\$p > svg > foreignObject > section:after{--marpit-root-font-size:24px;}div#\:\$p > svg > foreignObject > section[data-color] :is(h1, marp-h1),div#\:\$p > svg > foreignObject > section[data-color] :is(h2, marp-h2),div#\:\$p > svg > foreignObject > section[data-color] :is(h3, marp-h3),div#\:\$p > svg > foreignObject > section[data-color] :is(h4, marp-h4),div#\:\$p > svg > foreignObject > section[data-color] :is(h5, marp-h5),div#\:\$p > svg > foreignObject > section[data-color] :is(h6, marp-h6){color:currentcolor}div#\:\$p > svg > foreignObject > section{font-size:1.5em}div#\:\$p > svg > foreignObject > section{--marpit-root-font-size: 1.5em;}div#\:\$p > svg > foreignObject > section :is(h1, marp-h1){color:#0078D7}div#\:\$p > svg > foreignObject > section :is(h2, marp-h2){color:#00BCF2}div#\:\$p > svg > foreignObject > section code{background-color:#f0f0f0;padding:0.2em 0.4em;border-radius:3px}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"]{columns:initial!important;display:block!important;padding:0!important}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"]::before, div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"]::after, div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="content"]::before, div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="content"]::after{display:none!important}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"] > div[data-marpit-advanced-background-container]{all:initial;display:flex;flex-direction:row;height:100%;overflow:hidden;width:100%}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"] > div[data-marpit-advanced-background-container][data-marpit-advanced-background-direction="vertical"]{flex-direction:column}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"][data-marpit-advanced-background-split] > div[data-marpit-advanced-background-container]{width:var(--marpit-advanced-background-split, 50%)}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"][data-marpit-advanced-background-split="right"] > div[data-marpit-advanced-background-container]{margin-left:calc(100% - var(--marpit-advanced-background-split, 50%))}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"] > div[data-marpit-advanced-background-container] > figure{all:initial;background-position:center;background-repeat:no-repeat;background-size:cover;flex:auto;margin:0}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="background"] > div[data-marpit-advanced-background-container] > figure > figcaption{position:absolute;border:0;clip:rect(0, 0, 0, 0);height:1px;margin:-1px;overflow:hidden;padding:0;white-space:nowrap;width:1px}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="content"], div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="pseudo"]{background:transparent!important}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background="pseudo"], div#\:\$p > svg[data-marpit-svg] > foreignObject[data-marpit-advanced-background="pseudo"]{pointer-events:none!important}div#\:\$p > svg > foreignObject > section[data-marpit-advanced-background-split]{width:100%;height:100%}
</style></head><body><div class="bespoke-marp-osc"><button data-bespoke-marp-osc="prev" tabindex="-1" title="Previous slide">Previous slide</button><span data-bespoke-marp-osc="page"></span><button data-bespoke-marp-osc="next" tabindex="-1" title="Next slide">Next slide</button><button data-bespoke-marp-osc="fullscreen" tabindex="-1" title="Toggle fullscreen (f)">Toggle fullscreen</button><button data-bespoke-marp-osc="presenter" tabindex="-1" title="Open presenter view (p)">Open presenter view</button></div><div id=":$p"><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="1" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:40%;" data-marpit-pagination-total="29" data-marpit-advanced-background="background" data-marpit-advanced-background-split="right"><div data-marpit-advanced-background-container="true" data-marpit-advanced-background-direction="horizontal"><figure style="background-image:url(&quot;images/transformer_blackbox.avif&quot;);background-size:80%;"></figure></div></section></foreignObject><foreignObject width="60%" height="720"><section id="1" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="1" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:40%;" data-marpit-pagination-total="29" data-marpit-advanced-background="content" data-marpit-advanced-background-split="right">
<h1 id="transformer-e-gpt-dalla-teoria-allimplementazione-%F0%9F%9A%80">Transformer e GPT: Dalla teoria all'implementazione <img class="emoji" draggable="false" alt="🚀" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f680.svg" data-marp-twemoji=""/></h1>

<p><strong>Obiettivi della presentazione:</strong></p>
<ul>
<li>Capire i concetti chiave dei Transformer</li>
<li>Analizzare i componenti fondamentali dell'architettura</li>
<li>Approfondire l'architettura GPT e la sua implementazione</li>
<li>Vedere esempi pratici di codice e ottimizzazioni</li>
<li>Confrontare BERT e GPT e discutere applicazioni</li>
</ul>
<blockquote>
<p>Chi ha già usato modelli come ChatGPT o BERT? In che contesti?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject><foreignObject width="1280" height="720" data-marpit-advanced-background="pseudo"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="1" style="" data-marpit-pagination-total="29" data-marpit-advanced-background="pseudo" data-marpit-advanced-background-split="right"></section></foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="2" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="2" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="perch%C3%A9-i-transformer-hanno-rivoluzionato-lnlp">Perché i Transformer hanno rivoluzionato l'NLP?</h2>
<ul>
<li>Elaborazione <strong>parallela</strong> delle sequenze (non più sequenziale come le RNN)</li>
<li><strong>Self-attention</strong>: relazioni dirette tra tutte le parole della sequenza</li>
<li>Encoding posizionale esplicito per mantenere l’ordine</li>
<li><strong>Scalabilità</strong>: modelli sempre più grandi e potenti</li>
</ul>
<blockquote>
<p>Domanda: Quali problemi delle RNN vengono superati dai Transformer?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="3" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="3" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="quiz-cosa-rende-speciale-i-transformer-%F0%9F%A7%A9">Quiz: Cosa rende speciale i Transformer? <img class="emoji" draggable="false" alt="🧩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f9e9.svg" data-marp-twemoji=""/></h2>
<p>A) Usano solo reti convoluzionali<br />
B) Processano sequenze elemento per elemento<br />
C) Si basano sul meccanismo di self-attention<br />
D) Richiedono meno dati per l’addestramento</p>
<blockquote>
<p>Qual è la risposta corretta? Perché?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="4" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:50%;" data-marpit-pagination-total="29" data-marpit-advanced-background="background" data-marpit-advanced-background-split="right"><div data-marpit-advanced-background-container="true" data-marpit-advanced-background-direction="horizontal"><figure style="background-image:url(&quot;images/encoder_decoder_structure.avif&quot;);background-size:90%;"></figure></div></section></foreignObject><foreignObject width="50%" height="720"><section id="4" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="4" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:50%;" data-marpit-pagination-total="29" data-marpit-advanced-background="content" data-marpit-advanced-background-split="right">
<h2 id="struttura-generale-del-transformer">Struttura generale del Transformer</h2>

<ul>
<li><strong>Encoder</strong>: elabora il contesto dell’input (usato in BERT)</li>
<li><strong>Decoder</strong>: genera una sequenza in modo autoregressivo (usato in GPT)</li>
<li><strong>Self-attention</strong>: permette interazione tra tutte le posizioni</li>
<li><strong>Feed-Forward</strong>: trasforma le rappresentazioni posizione per posizione</li>
<li><strong>LayerNorm &amp; Residual</strong>: stabilità e profondità</li>
</ul>
<blockquote>
<p>Secondo voi, qual è il componente più determinante per il successo dei Transformer?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject><foreignObject width="1280" height="720" data-marpit-advanced-background="pseudo"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="4" style="" data-marpit-pagination-total="29" data-marpit-advanced-background="pseudo" data-marpit-advanced-background-split="right"></section></foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="5" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="5" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="tabella-dei-componenti-chiave">Tabella dei componenti chiave</h2>
<table>
<thead>
<tr>
<th>Componente</th>
<th>Descrizione</th>
<th>Scopo principale</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multi-Head Attention</td>
<td>Più &quot;canali&quot; di attenzione</td>
<td>Cattura molteplici relazioni tra posizioni</td>
</tr>
<tr>
<td>Feed-Forward Network</td>
<td>Strati densi per posizione</td>
<td>Aumenta la complessità e la capacità del modello</td>
</tr>
<tr>
<td>Positional Encoding</td>
<td>Informazione di posizione</td>
<td>Permette di distinguere l’ordine dei token</td>
</tr>
<tr>
<td>Layer Normalization</td>
<td>Normalizza ogni sotto-strato</td>
<td>Stabilizza e accelera l’addestramento</td>
</tr>
<tr>
<td>Residual Connection</td>
<td>Collegamenti diretti tra strati</td>
<td>Facilita il flusso del gradiente nelle reti profonde</td>
</tr>
<tr>
<td>Dropout</td>
<td>Disattiva connessioni in modo casuale</td>
<td>Riduce l’overfitting</td>
</tr>
</tbody>
</table>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="6" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="6" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="embedding-rappresentare-i-token-come-vettori">Embedding: rappresentare i token come vettori</h2>
<p><img src="images/embedding.avif" alt="alt text" /></p>
<p>Ogni parola/token viene trasformato in un vettore continuo (embedding) che il modello può elaborare.</p>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="7" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:40%;" data-marpit-pagination-total="29" data-marpit-advanced-background="background" data-marpit-advanced-background-split="right"><div data-marpit-advanced-background-container="true" data-marpit-advanced-background-direction="horizontal"><figure style="background-image:url(&quot;images/Multi_Head_Attention.avif&quot;);background-size:90%;"></figure></div></section></foreignObject><foreignObject width="60%" height="720"><section id="7" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="7" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:40%;" data-marpit-pagination-total="29" data-marpit-advanced-background="content" data-marpit-advanced-background-split="right">
<h2 id="il-cuore-del-transformer-self-attention-%E2%9D%A4%EF%B8%8F">Il cuore del Transformer: Self-Attention <img class="emoji" draggable="false" alt="❤️" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/2764.svg" data-marp-twemoji=""/></h2>

<p><strong>Query, Key, Value per ogni token:</strong></p>
<ul>
<li><strong>Query (Q):</strong> Cosa sta cercando il token?</li>
<li><strong>Key (K):</strong> Cosa offre agli altri token?</li>
<li><strong>Value (V):</strong> Qual è il contenuto da trasmettere?</li>
</ul>
<blockquote>
<p>Come aiuta questo meccanismo a cogliere relazioni tra parole anche lontane nella frase?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject><foreignObject width="1280" height="720" data-marpit-advanced-background="pseudo"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="7" style="" data-marpit-pagination-total="29" data-marpit-advanced-background="pseudo" data-marpit-advanced-background-split="right"></section></foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="8" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:40%;" data-marpit-pagination-total="29" data-marpit-advanced-background="background" data-marpit-advanced-background-split="right"><div data-marpit-advanced-background-container="true" data-marpit-advanced-background-direction="horizontal"><figure style="background-image:url(&quot;images/softmax_adjusted_scores.avif&quot;);background-size:90%;"></figure></div></section></foreignObject><foreignObject width="60%" height="720"><section id="8" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="8" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:40%;" data-marpit-pagination-total="29" data-marpit-advanced-background="content" data-marpit-advanced-background-split="right">
<h2 id="calcolo-dellattenzione-i-4-step-chiave">Calcolo dell'attenzione: i 4 step chiave</h2>
<ol>
<li><strong>Scores = Q × Kᵗ</strong></li>
<li><strong>Scaling:</strong> Scores / √dₖ</li>
<li><strong>Softmax:</strong> pesi di attenzione</li>
<li><strong>Output:</strong> somma pesata dei Value</li>
</ol>

<blockquote>
<p>Perché dividiamo per √dₖ? Cosa accadrebbe senza scaling?</p>
</blockquote>
<p><strong>Multi-Head Attention:</strong><br />
Più &quot;teste&quot; di attenzione in parallelo → ogni testa può imparare relazioni diverse (es. sintattiche, semantiche, a lungo termine).</p>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject><foreignObject width="1280" height="720" data-marpit-advanced-background="pseudo"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="8" style="" data-marpit-pagination-total="29" data-marpit-advanced-background="pseudo" data-marpit-advanced-background-split="right"></section></foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="9" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:50%;" data-marpit-pagination-total="29" data-marpit-advanced-background="background" data-marpit-advanced-background-split="right"><div data-marpit-advanced-background-container="true" data-marpit-advanced-background-direction="horizontal"><figure style="background-image:url(&quot;images/positional-encoding.avif&quot;);background-size:90%;"></figure></div></section></foreignObject><foreignObject width="50%" height="720"><section id="9" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="9" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:50%;" data-marpit-pagination-total="29" data-marpit-advanced-background="content" data-marpit-advanced-background-split="right">
<h2 id="positional-encoding-come-dare-il-senso-dellordine-%F0%9F%93%8D">Positional Encoding: come dare il senso dell’ordine <img class="emoji" draggable="false" alt="📍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f4cd.svg" data-marp-twemoji=""/></h2>

<ul>
<li>Il Transformer non &quot;vede&quot; l’ordine dei token: serve un encoding posizionale.</li>
<li>Si usano funzioni sinusoidali di diverse frequenze per ogni posizione e dimensione.</li>
</ul>
<blockquote>
<p>Perché è utile usare funzioni periodiche invece di numeri sequenziali?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject><foreignObject width="1280" height="720" data-marpit-advanced-background="pseudo"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="9" style="" data-marpit-pagination-total="29" data-marpit-advanced-background="pseudo" data-marpit-advanced-background-split="right"></section></foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="10" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:50%;" data-marpit-pagination-total="29" data-marpit-advanced-background="background" data-marpit-advanced-background-split="right"><div data-marpit-advanced-background-container="true" data-marpit-advanced-background-direction="horizontal"><figure style="background-image:url(&quot;images/encoder.avif&quot;);background-size:70%;"></figure></div></section></foreignObject><foreignObject width="50%" height="720"><section id="10" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="10" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:50%;" data-marpit-pagination-total="29" data-marpit-advanced-background="content" data-marpit-advanced-background-split="right">
<h2 id="feed-forward-e-layer-normalization">Feed-Forward e Layer Normalization</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-comment"># Feed-Forward Network</span>
FFN(x) = <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, x * W_1 + b_1) * W_2 + b_2

<span class="hljs-comment"># Layer Normalization</span>
LayerNorm(x) = γ * (x - μ) / √(σ² + ε) + β
</code></pre>
<ul>
<li><strong>Feed-Forward:</strong> trasforma ogni posizione in modo indipendente, aggiunge non-linearità.</li>
<li><strong>LayerNorm:</strong> normalizza ogni esempio → stabilità anche con sequenze di lunghezza variabile.</li>
<li><strong>Residual:</strong> somma input e output di ogni blocco.</li>
</ul>
<blockquote>
<p>Perché LayerNorm è preferita a BatchNorm nei Transformer?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject><foreignObject width="1280" height="720" data-marpit-advanced-background="pseudo"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="10" style="" data-marpit-pagination-total="29" data-marpit-advanced-background="pseudo" data-marpit-advanced-background-split="right"></section></foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="11" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:40%;" data-marpit-pagination-total="29" data-marpit-advanced-background="background" data-marpit-advanced-background-split="right"><div data-marpit-advanced-background-container="true" data-marpit-advanced-background-direction="horizontal"><figure style="background-image:url(&quot;images/decoder_structure.png&quot;);background-size:90%;"></figure></div></section></foreignObject><foreignObject width="60%" height="720"><section id="11" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="11" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:40%;" data-marpit-pagination-total="29" data-marpit-advanced-background="content" data-marpit-advanced-background-split="right">
<h2 id="decoder-transformer-la-base-di-gpt-%F0%9F%A7%A0">Decoder Transformer: la base di GPT <img class="emoji" draggable="false" alt="🧠" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f9e0.svg" data-marp-twemoji=""/></h2>

<ul>
<li><strong>Masked self-attention:</strong> ogni token vede solo i precedenti</li>
<li>Stack di blocchi identici: attention, feed-forward, LayerNorm, residual</li>
<li>Output: layer lineare + softmax (predizione della parola successiva)</li>
</ul>
<blockquote>
<p>Perché è fondamentale mascherare le posizioni future nel decoder?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject><foreignObject width="1280" height="720" data-marpit-advanced-background="pseudo"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="11" style="" data-marpit-pagination-total="29" data-marpit-advanced-background="pseudo" data-marpit-advanced-background-split="right"></section></foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="12" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:30%;" data-marpit-pagination-total="29" data-marpit-advanced-background="background" data-marpit-advanced-background-split="right"><div data-marpit-advanced-background-container="true" data-marpit-advanced-background-direction="horizontal"><figure style="background-image:url(&quot;https://pytorch.org/assets/images/pytorch-logo.png&quot;);background-size:90%;"></figure></div></section></foreignObject><foreignObject width="70%" height="720"><section id="12" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="12" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:30%;" data-marpit-pagination-total="29" data-marpit-advanced-background="content" data-marpit-advanced-background-split="right">
<h2 id="implementazione-gpt-step-by-step-%F0%9F%92%BB">Implementazione GPT step-by-step <img class="emoji" draggable="false" alt="💻" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f4bb.svg" data-marp-twemoji=""/></h2>

<p><strong>Cosa vedremo:</strong></p>
<ul>
<li>Analisi dei componenti chiave in codice PyTorch</li>
<li>Implementazione di un mini-GPT da zero</li>
<li>Esempio pratico di forward, loss e generazione</li>
</ul>
<blockquote>
<p>Chi ha già usato PyTorch o implementato modelli deep learning da zero?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject><foreignObject width="1280" height="720" data-marpit-advanced-background="pseudo"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="12" style="" data-marpit-pagination-total="29" data-marpit-advanced-background="pseudo" data-marpit-advanced-background-split="right"></section></foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="13" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="13" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="setup-e-configurazione-la-classe-di-base">Setup e configurazione: la classe di base</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F

<span class="hljs-keyword">class</span> <span class="hljs-title class_">GPTConfig</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, block_size, n_embd=<span class="hljs-number">768</span>, 
                 n_layer=<span class="hljs-number">12</span>, n_head=<span class="hljs-number">12</span>, dropout=<span class="hljs-number">0.1</span></span>):
        <span class="hljs-variable language_">self</span>.vocab_size = vocab_size  <span class="hljs-comment"># dimensione del vocabolario</span>
        <span class="hljs-variable language_">self</span>.block_size = block_size  <span class="hljs-comment"># lunghezza massima della sequenza</span>
        <span class="hljs-variable language_">self</span>.n_embd = n_embd          <span class="hljs-comment"># dimensione degli embedding</span>
        <span class="hljs-variable language_">self</span>.n_layer = n_layer        <span class="hljs-comment"># numero di blocchi del decoder</span>
        <span class="hljs-variable language_">self</span>.n_head = n_head          <span class="hljs-comment"># numero di teste di attenzione</span>
        <span class="hljs-variable language_">self</span>.dropout = dropout        <span class="hljs-comment"># probabilità di dropout</span>
</code></pre>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="14" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="14" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="self-attention-implementazione-in-pytorch">Self-Attention: implementazione in PyTorch</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-keyword">assert</span> config.n_embd % config.n_head == <span class="hljs-number">0</span>
        <span class="hljs-comment"># key, query, value projections</span>
        <span class="hljs-variable language_">self</span>.key = nn.Linear(config.n_embd, config.n_embd)
        <span class="hljs-variable language_">self</span>.query = nn.Linear(config.n_embd, config.n_embd)
        <span class="hljs-variable language_">self</span>.value = nn.Linear(config.n_embd, config.n_embd)
        <span class="hljs-comment"># output projection</span>
        <span class="hljs-variable language_">self</span>.proj = nn.Linear(config.n_embd, config.n_embd)
        <span class="hljs-comment"># regularization</span>
        <span class="hljs-variable language_">self</span>.attn_dropout = nn.Dropout(config.dropout)
        <span class="hljs-variable language_">self</span>.resid_dropout = nn.Dropout(config.dropout)
        <span class="hljs-comment"># causal mask</span>
        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;mask&quot;</span>, torch.tril(torch.ones(config.block_size, 
                             config.block_size)).view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, config.block_size, config.block_size))
        <span class="hljs-variable language_">self</span>.n_head = config.n_head
        <span class="hljs-variable language_">self</span>.n_embd = config.n_embd
</code></pre>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="15" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="15" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="self-attention-forward-pass">Self-Attention: forward pass</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
    B, T, C = x.size() <span class="hljs-comment"># batch size, sequence length, embedding dimensionality</span>
    
    <span class="hljs-comment"># calculate query, key, values for all heads in batch</span>
    k = <span class="hljs-variable language_">self</span>.key(x).view(B, T, <span class="hljs-variable language_">self</span>.n_head, C // <span class="hljs-variable language_">self</span>.n_head).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># (B, nh, T, hs)</span>
    q = <span class="hljs-variable language_">self</span>.query(x).view(B, T, <span class="hljs-variable language_">self</span>.n_head, C // <span class="hljs-variable language_">self</span>.n_head).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># (B, nh, T, hs)</span>
    v = <span class="hljs-variable language_">self</span>.value(x).view(B, T, <span class="hljs-variable language_">self</span>.n_head, C // <span class="hljs-variable language_">self</span>.n_head).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># (B, nh, T, hs)</span>
    
    <span class="hljs-comment"># causal self-attention: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span>
    att = (q @ k.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) * (<span class="hljs-number">1.0</span> / math.sqrt(k.size(-<span class="hljs-number">1</span>)))
    att = att.masked_fill(<span class="hljs-variable language_">self</span>.mask[:,:,:T,:T] == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>))
    att = F.softmax(att, dim=-<span class="hljs-number">1</span>)
    att = <span class="hljs-variable language_">self</span>.attn_dropout(att)
    y = att @ v <span class="hljs-comment"># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span>
    y = y.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(B, T, C) <span class="hljs-comment"># re-assemble all head outputs side by side</span>
    
    <span class="hljs-comment"># output projection</span>
    y = <span class="hljs-variable language_">self</span>.resid_dropout(<span class="hljs-variable language_">self</span>.proj(y))
    <span class="hljs-keyword">return</span> y
</code></pre>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="16" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="16" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="feed-forward-network-codice-essenziale">Feed-Forward Network: codice essenziale</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedForward</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.net = nn.Sequential(
            nn.Linear(config.n_embd, <span class="hljs-number">4</span> * config.n_embd),
            nn.GELU(),  <span class="hljs-comment"># Utilizziamo GELU invece di ReLU come in GPT</span>
            nn.Linear(<span class="hljs-number">4</span> * config.n_embd, config.n_embd),
            nn.Dropout(config.dropout),
        )

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.net(x)
</code></pre>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="17" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="17" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="un-blocco-transformer-combinare-attention-e-feed-forward">Un blocco Transformer: combinare attention e feed-forward</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Block</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.ln1 = nn.LayerNorm(config.n_embd)
        <span class="hljs-variable language_">self</span>.attn = SelfAttention(config)
        <span class="hljs-variable language_">self</span>.ln2 = nn.LayerNorm(config.n_embd)
        <span class="hljs-variable language_">self</span>.ffn = FeedForward(config)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-comment"># Utilizziamo la normalizzazione pre-layer come in GPT-2</span>
        x = x + <span class="hljs-variable language_">self</span>.attn(<span class="hljs-variable language_">self</span>.ln1(x))
        x = x + <span class="hljs-variable language_">self</span>.ffn(<span class="hljs-variable language_">self</span>.ln2(x))
        <span class="hljs-keyword">return</span> x
</code></pre>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="18" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="18" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="modello-gpt-completo-struttura-principale">Modello GPT completo: struttura principale</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPT</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.config = config
        
        <span class="hljs-comment"># input embedding</span>
        <span class="hljs-variable language_">self</span>.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)
        <span class="hljs-variable language_">self</span>.pos_emb = nn.Parameter(torch.zeros(<span class="hljs-number">1</span>, config.block_size, config.n_embd))
        <span class="hljs-variable language_">self</span>.drop = nn.Dropout(config.dropout)
        
        <span class="hljs-comment"># transformer blocks</span>
        <span class="hljs-variable language_">self</span>.blocks = nn.Sequential(*[Block(config) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.n_layer)])
        
        <span class="hljs-comment"># final layer norm</span>
        <span class="hljs-variable language_">self</span>.ln_f = nn.LayerNorm(config.n_embd)
        
        <span class="hljs-comment"># language modeling head</span>
        <span class="hljs-variable language_">self</span>.head = nn.Linear(config.n_embd, config.vocab_size, bias=<span class="hljs-literal">False</span>)
        
        <span class="hljs-comment"># initialize weights</span>
        <span class="hljs-variable language_">self</span>.apply(<span class="hljs-variable language_">self</span>._init_weights)
</code></pre>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="19" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="19" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="modello-gpt-completo-pesi-e-forward-pass">Modello GPT completo: pesi e forward pass</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_weights</span>(<span class="hljs-params">self, module</span>):
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, (nn.Linear, nn.Embedding)):
        module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear) <span class="hljs-keyword">and</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            module.bias.data.zero_()
    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(module, nn.LayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(<span class="hljs-number">1.0</span>)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, idx, targets=<span class="hljs-literal">None</span></span>):
    B, T = idx.size()
    <span class="hljs-keyword">assert</span> T &lt;= <span class="hljs-variable language_">self</span>.config.block_size, <span class="hljs-string">f&quot;Cannot forward sequence of length <span class="hljs-subst">{T}</span>, block size is only <span class="hljs-subst">{self.config.block_size}</span>&quot;</span>
    
    <span class="hljs-comment"># forward the GPT model</span>
    token_embeddings = <span class="hljs-variable language_">self</span>.tok_emb(idx) <span class="hljs-comment"># (B, T, n_embd)</span>
    position_embeddings = <span class="hljs-variable language_">self</span>.pos_emb[:, :T, :] <span class="hljs-comment"># (1, T, n_embd)</span>
    x = <span class="hljs-variable language_">self</span>.drop(token_embeddings + position_embeddings)
    x = <span class="hljs-variable language_">self</span>.blocks(x)
    x = <span class="hljs-variable language_">self</span>.ln_f(x)
    logits = <span class="hljs-variable language_">self</span>.head(x) <span class="hljs-comment"># (B, T, vocab_size)</span>
    
    <span class="hljs-comment"># if we are given some desired targets also calculate the loss</span>
    loss = <span class="hljs-literal">None</span>
    <span class="hljs-keyword">if</span> targets <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        loss = F.cross_entropy(logits.view(-<span class="hljs-number">1</span>, logits.size(-<span class="hljs-number">1</span>)), targets.view(-<span class="hljs-number">1</span>))
    
    <span class="hljs-keyword">return</span> logits, loss
</code></pre>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="20" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="20" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="generazione-di-testo-autoregressiva">Generazione di testo autoregressiva</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">self, idx, max_new_tokens, temperature=<span class="hljs-number">1.0</span>, top_k=<span class="hljs-literal">None</span></span>):
    <span class="hljs-string">&quot;&quot;&quot;
    Generate new tokens beyond the context provided in idx.
    
    Args:
        idx: (B, T) tensor of indices in the current context
        max_new_tokens: number of new tokens to generate
        temperature: softmax temperature, higher values increase diversity
        top_k: if set, only sample from the top k most probable tokens
    
    Returns:
        (B, T+max_new_tokens) tensor of indices
    &quot;&quot;&quot;</span>
    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):
        <span class="hljs-comment"># crop context if needed</span>
        idx_cond = idx <span class="hljs-keyword">if</span> idx.size(<span class="hljs-number">1</span>) &lt;= <span class="hljs-variable language_">self</span>.config.block_size <span class="hljs-keyword">else</span> idx[:, -<span class="hljs-variable language_">self</span>.config.block_size:]
        <span class="hljs-comment"># forward pass</span>
        logits, _ = <span class="hljs-variable language_">self</span>.forward(idx_cond)
        <span class="hljs-comment"># focus on the last time step</span>
        logits = logits[:, -<span class="hljs-number">1</span>, :] / temperature
        <span class="hljs-comment"># optionally crop probabilities to only the top k options</span>
        <span class="hljs-keyword">if</span> top_k <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            v, _ = torch.topk(logits, <span class="hljs-built_in">min</span>(top_k, logits.size(-<span class="hljs-number">1</span>)))
            logits[logits &lt; v[:, [-<span class="hljs-number">1</span>]]] = -<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;Inf&#x27;</span>)
        <span class="hljs-comment"># apply softmax to convert to probabilities</span>
        probs = F.softmax(logits, dim=-<span class="hljs-number">1</span>)
        <span class="hljs-comment"># sample from the distribution</span>
        idx_next = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>)
        <span class="hljs-comment"># append sampled index to the running sequence</span>
        idx = torch.cat((idx, idx_next), dim=<span class="hljs-number">1</span>)
            
    <span class="hljs-keyword">return</span> idx
</code></pre>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="21" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="21" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="esempio-pratico-come-usare-il-modello-gpt">Esempio pratico: come usare il modello GPT</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-comment"># Esempio di configurazione per un modello piccolo</span>
config = GPTConfig(
    vocab_size=<span class="hljs-number">50257</span>,  <span class="hljs-comment"># Dimensione del vocabolario GPT-2</span>
    block_size=<span class="hljs-number">1024</span>,   <span class="hljs-comment"># Lunghezza massima della sequenza</span>
    n_embd=<span class="hljs-number">768</span>,        <span class="hljs-comment"># Dimensione degli embedding</span>
    n_layer=<span class="hljs-number">12</span>,        <span class="hljs-comment"># Numero di blocchi</span>
    n_head=<span class="hljs-number">12</span>,         <span class="hljs-comment"># Numero di teste di attenzione</span>
    dropout=<span class="hljs-number">0.1</span>        <span class="hljs-comment"># Dropout</span>
)

<span class="hljs-comment"># Inizializzazione del modello</span>
model = GPT(config)

<span class="hljs-comment"># Esempio di input (batch di 2 sequenze di 10 token ciascuna)</span>
x = torch.randint(<span class="hljs-number">0</span>, config.vocab_size, (<span class="hljs-number">2</span>, <span class="hljs-number">10</span>))
targets = torch.randint(<span class="hljs-number">0</span>, config.vocab_size, (<span class="hljs-number">2</span>, <span class="hljs-number">10</span>))

<span class="hljs-comment"># Forward pass</span>
logits, loss = model(x, targets)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Input shape: <span class="hljs-subst">{x.shape}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Logits shape: <span class="hljs-subst">{logits.shape}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Loss: <span class="hljs-subst">{loss.item()}</span>&quot;</span>)

<span class="hljs-comment"># Generazione di testo</span>
context = torch.tensor([[<span class="hljs-number">464</span>, <span class="hljs-number">1842</span>, <span class="hljs-number">2181</span>, <span class="hljs-number">1447</span>, <span class="hljs-number">373</span>]])  <span class="hljs-comment"># Esempio di contesto iniziale</span>
generated = model.generate(context, max_new_tokens=<span class="hljs-number">20</span>, temperature=<span class="hljs-number">0.8</span>, top_k=<span class="hljs-number">40</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated sequence shape: <span class="hljs-subst">{generated.shape}</span>&quot;</span>)
</code></pre>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="22" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="22" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="addestrare-il-modello-ciclo-base">Addestrare il modello: ciclo base</h2>
<pre is="marp-pre" data-auto-scaling="downscale-only"><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model, data_loader, optimizer, epochs, device</span>):
    model.train()
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
        total_loss = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> batch_idx, (x, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):
            x, y = x.to(device), y.to(device)
            
            <span class="hljs-comment"># Forward pass</span>
            optimizer.zero_grad()
            logits, loss = model(x, y)
            
            <span class="hljs-comment"># Backward pass</span>
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            <span class="hljs-keyword">if</span> batch_idx % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>/<span class="hljs-subst">{epochs}</span>, Batch <span class="hljs-subst">{batch_idx}</span>, Loss: <span class="hljs-subst">{loss.item():<span class="hljs-number">.4</span>f}</span>&quot;</span>)
        
        avg_loss = total_loss / <span class="hljs-built_in">len</span>(data_loader)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>/<span class="hljs-subst">{epochs}</span>, Average Loss: <span class="hljs-subst">{avg_loss:<span class="hljs-number">.4</span>f}</span>&quot;</span>)
</code></pre>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="23" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="23" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="ottimizzazioni-pratiche-e-best-practices">Ottimizzazioni pratiche e best practices</h2>
<ol>
<li>
<p><strong>Efficienza computazionale</strong>:</p>
<ul>
<li>Mixed precision training</li>
<li>Gradient accumulation</li>
<li>Model parallelism</li>
</ul>
</li>
<li>
<p><strong>Tokenizzazione</strong>: Byte-Pair Encoding (BPE)</p>
</li>
<li>
<p><strong>Strategie di addestramento</strong>:</p>
<ul>
<li>Learning rate scheduling</li>
<li>Weight decay</li>
<li>Gradient clipping</li>
</ul>
</li>
<li>
<p><strong>Generazione di testo</strong>:</p>
<ul>
<li>Beam search</li>
<li>Nucleus sampling</li>
<li>Temperature scaling</li>
</ul>
</li>
</ol>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="24" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="24" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="bert-vs-gpt-differenze-chiave">BERT vs GPT: differenze chiave</h2>
<table>
<thead>
<tr>
<th>Caratteristica</th>
<th>BERT</th>
<th>GPT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Architettura</strong></td>
<td>Encoder</td>
<td>Decoder</td>
</tr>
<tr>
<td><strong>Direzionalità</strong></td>
<td>Bidirezionale</td>
<td>Unidirezionale (left-to-right)</td>
</tr>
<tr>
<td><strong>Pre-addestramento</strong></td>
<td>Masked Language Modeling (MLM)</td>
<td>Predizione della parola successiva</td>
</tr>
<tr>
<td><strong>Applicazioni tipiche</strong></td>
<td>Comprensione (classificazione, NER, QA)</td>
<td>Generazione (completamento, traduzione, riassunto)</td>
</tr>
</tbody>
</table>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="25" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:40%;" data-marpit-pagination-total="29" data-marpit-advanced-background="background" data-marpit-advanced-background-split="right"><div data-marpit-advanced-background-container="true" data-marpit-advanced-background-direction="horizontal"><figure style="background-image:url(&quot;https://media.giphy.com/media/3o7buirYcmV5nSwIRW/giphy.gif&quot;);background-size:80%;"></figure></div></section></foreignObject><foreignObject width="60%" height="720"><section id="25" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="25" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;--marpit-advanced-background-split:40%;" data-marpit-pagination-total="29" data-marpit-advanced-background="content" data-marpit-advanced-background-split="right">
<h2 id="domande--discussione-finale-%F0%9F%A4%94">Domande &amp; Discussione finale <img class="emoji" draggable="false" alt="🤔" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f914.svg" data-marp-twemoji=""/></h2>

<p><strong>Hai dubbi sull'architettura o l'implementazione?</strong></p>
<ul>
<li>Differenze con LSTM/CNN?</li>
<li>Limiti attuali dei Transformer?</li>
<li>Come adattare GPT a un task specifico?</li>
<li>Implicazioni etiche dei LLM?</li>
<li>Come interpretare cosa &quot;vede&quot; un Transformer?</li>
</ul>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject><foreignObject width="1280" height="720" data-marpit-advanced-background="pseudo"><section data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="25" style="" data-marpit-pagination-total="29" data-marpit-advanced-background="pseudo" data-marpit-advanced-background-split="right"></section></foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="26" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="26" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="conclusioni-e-takeaway-%F0%9F%8E%AF">Conclusioni e takeaway <img class="emoji" draggable="false" alt="🎯" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f3af.svg" data-marp-twemoji=""/></h2>
<ul>
<li>Il Transformer ha rivoluzionato l’NLP grazie a self-attention e parallelizzazione</li>
<li>GPT sfrutta il decoder per la generazione di testo</li>
<li>L’implementazione da zero aiuta a capire ogni componente</li>
<li>Questi modelli sono la base di ChatGPT, LLM e molte applicazioni attuali</li>
</ul>
<p><strong>Riferimenti principali:</strong></p>
<ul>
<li>Vaswani et al. (2017), &quot;Attention is All You Need&quot;</li>
<li>Radford et al. (2018, 2019), &quot;Generative Pre-Training&quot;</li>
</ul>
<blockquote>
<p>Come immagini il futuro dei modelli linguistici? Quali nuove applicazioni potrebbero nascere?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="27" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="27" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="una-domanda-per-il-futuro-%F0%9F%A4%94">Una domanda per il futuro... <img class="emoji" draggable="false" alt="🤔" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f914.svg" data-marp-twemoji=""/></h2>
<p>Se un modello GPT potesse scrivere il suo stesso codice, come si implementerebbe?</p>
<blockquote>
<p>Riflettiamo: l’IA può contribuire al proprio sviluppo?</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="28" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="28" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="risorse-per-approfondire-%F0%9F%93%9A">Risorse per approfondire <img class="emoji" draggable="false" alt="📚" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f4da.svg" data-marp-twemoji=""/></h2>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> (Jay Alammar)</li>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> (Harvard NLP)</li>
<li><a href="https://github.com/karpathy/minGPT">minGPT</a> (Andrej Karpathy)</li>
<li><a href="https://huggingface.co/docs/transformers/index">Hugging Face Transformers</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need (paper)</a></li>
</ul>
<blockquote>
<p>Consiglio: partite da &quot;The Illustrated Transformer&quot; per una spiegazione visiva!</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
</foreignObject></svg><svg data-marpit-svg="" viewBox="0 0 1280 720"><foreignObject width="1280" height="720"><section id="29" data-paginate="true" data-footer="Corso NLP - Appendice: Transformer e GPT" data-theme="default" data-style="section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
" lang="en-US" data-marpit-pagination="29" style="--paginate:true;--footer:Corso NLP - Appendice: Transformer e GPT;--theme:default;--style:section {
  font-size: 1.5em;
}
h1 {
  color: #0078D7;
}
h2 {
  color: #00BCF2;
}
code {
  background-color: #f0f0f0;
  padding: 0.2em 0.4em;
  border-radius: 3px;
}
;" data-marpit-pagination-total="29">
<h2 id="quiz-e-domande-interattive-%F0%9F%8E%B2">Quiz e domande interattive <img class="emoji" draggable="false" alt="🎲" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f3b2.svg" data-marp-twemoji=""/></h2>
<p><strong>1. Cosa distingue i Transformer dalle RNN?</strong><br />
<strong>2. Perché serve il positional encoding?</strong><br />
<strong>3. Qual è il ruolo della maschera nel decoder di GPT?</strong><br />
<strong>4. In quali task useresti BERT invece di GPT?</strong><br />
<strong>5. Come influenzano temperature e top_k la generazione di testo?</strong></p>
<blockquote>
<p>Discutiamone insieme!</p>
</blockquote>
<footer>Corso NLP - Appendice: Transformer e GPT</footer>
</section>
<script>!function(){"use strict";const t={h1:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"1"},style:"display: block; font-size: 2em; margin-block-start: 0.67em; margin-block-end: 0.67em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},h2:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"2"},style:"display: block; font-size: 1.5em; margin-block-start: 0.83em; margin-block-end: 0.83em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},h3:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"3"},style:"display: block; font-size: 1.17em; margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},h4:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"4"},style:"display: block; margin-block-start: 1.33em; margin-block-end: 1.33em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},h5:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"5"},style:"display: block; font-size: 0.83em; margin-block-start: 1.67em; margin-block-end: 1.67em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},h6:{proto:()=>HTMLHeadingElement,attrs:{role:"heading","aria-level":"6"},style:"display: block; font-size: 0.67em; margin-block-start: 2.33em; margin-block-end: 2.33em; margin-inline-start: 0px; margin-inline-end: 0px; font-weight: bold;"},span:{proto:()=>HTMLSpanElement},pre:{proto:()=>HTMLElement,style:"display: block; font-family: monospace; white-space: pre; margin: 1em 0; --marp-auto-scaling-white-space: pre;"}},e="data-marp-auto-scaling-wrapper",i="data-marp-auto-scaling-svg",n="data-marp-auto-scaling-container";class s extends HTMLElement{container;containerSize;containerObserver;svg;svgComputedStyle;svgPreserveAspectRatio="xMinYMid meet";wrapper;wrapperSize;wrapperObserver;constructor(){super();const t=t=>([e])=>{const{width:i,height:n}=e.contentRect;this[t]={width:i,height:n},this.updateSVGRect()};this.attachShadow({mode:"open"}),this.containerObserver=new ResizeObserver(t("containerSize")),this.wrapperObserver=new ResizeObserver(((...e)=>{t("wrapperSize")(...e),this.flushSvgDisplay()}))}static get observedAttributes(){return["data-downscale-only"]}connectedCallback(){this.shadowRoot.innerHTML=`\n<style>\n  svg[${i}] { display: block; width: 100%; height: auto; vertical-align: top; }\n  span[${n}] { display: table; white-space: var(--marp-auto-scaling-white-space, nowrap); width: max-content; }\n</style>\n<div ${e}>\n  <svg part="svg" ${i}>\n    <foreignObject><span ${n}><slot></slot></span></foreignObject>\n  </svg>\n</div>\n    `.split(/\n\s*/).join(""),this.wrapper=this.shadowRoot.querySelector(`div[${e}]`)??void 0;const t=this.svg;this.svg=this.wrapper?.querySelector(`svg[${i}]`)??void 0,this.svg!==t&&(this.svgComputedStyle=this.svg?window.getComputedStyle(this.svg):void 0),this.container=this.svg?.querySelector(`span[${n}]`)??void 0,this.observe()}disconnectedCallback(){this.svg=void 0,this.svgComputedStyle=void 0,this.wrapper=void 0,this.container=void 0,this.observe()}attributeChangedCallback(){this.observe()}flushSvgDisplay(){const{svg:t}=this;t&&(t.style.display="inline",requestAnimationFrame((()=>{t.style.display=""})))}observe(){this.containerObserver.disconnect(),this.wrapperObserver.disconnect(),this.wrapper&&this.wrapperObserver.observe(this.wrapper),this.container&&this.containerObserver.observe(this.container),this.svgComputedStyle&&this.observeSVGStyle(this.svgComputedStyle)}observeSVGStyle(t){const e=()=>{const i=(()=>{const e=t.getPropertyValue("--preserve-aspect-ratio");if(e)return e.trim();return`x${(({textAlign:t,direction:e})=>{if(t.endsWith("left"))return"Min";if(t.endsWith("right"))return"Max";if("start"===t||"end"===t){let i="rtl"===e;return"end"===t&&(i=!i),i?"Max":"Min"}return"Mid"})(t)}YMid meet`})();i!==this.svgPreserveAspectRatio&&(this.svgPreserveAspectRatio=i,this.updateSVGRect()),t===this.svgComputedStyle&&requestAnimationFrame(e)};e()}updateSVGRect(){let t=Math.ceil(this.containerSize?.width??0);const e=Math.ceil(this.containerSize?.height??0);void 0!==this.dataset.downscaleOnly&&(t=Math.max(t,this.wrapperSize?.width??0));const i=this.svg?.querySelector(":scope > foreignObject");if(i?.setAttribute("width",`${t}`),i?.setAttribute("height",`${e}`),this.svg&&(this.svg.setAttribute("viewBox",`0 0 ${t} ${e}`),this.svg.setAttribute("preserveAspectRatio",this.svgPreserveAspectRatio),this.svg.style.height=t<=0||e<=0?"0":""),this.container){const t=this.svgPreserveAspectRatio.toLowerCase();this.container.style.marginLeft=t.startsWith("xmid")||t.startsWith("xmax")?"auto":"0",this.container.style.marginRight=t.startsWith("xmi")?"auto":"0"}}}const r=(t,{attrs:e={},style:i})=>class extends t{constructor(...t){super(...t);for(const[t,i]of Object.entries(e))this.hasAttribute(t)||this.setAttribute(t,i);this._shadow()}static get observedAttributes(){return["data-auto-scaling"]}connectedCallback(){this._update()}attributeChangedCallback(){this._update()}_shadow(){if(!this.shadowRoot)try{this.attachShadow({mode:"open"})}catch(t){if(!(t instanceof Error&&"NotSupportedError"===t.name))throw t}return this.shadowRoot}_update(){const t=this._shadow();if(t){const e=i?`<style>:host { ${i} }</style>`:"";let n="<slot></slot>";const{autoScaling:s}=this.dataset;if(void 0!==s){n=`<marp-auto-scaling exportparts="svg:auto-scaling" ${"downscale-only"===s?"data-downscale-only":""}>${n}</marp-auto-scaling>`}t.innerHTML=e+n}}};let o;const a=Symbol();let l;const c="marpitSVGPolyfill:setZoomFactor,",d=Symbol(),h=Symbol();const g=()=>{const t="Apple Computer, Inc."===navigator.vendor,e=t?[u]:[],i={then:e=>(t?(async()=>{if(void 0===l){const t=document.createElement("canvas");t.width=10,t.height=10;const e=t.getContext("2d"),i=new Image(10,10),n=new Promise((t=>{i.addEventListener("load",(()=>t()))}));i.crossOrigin="anonymous",i.src="data:image/svg+xml;charset=utf8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2210%22%20height%3D%2210%22%20viewBox%3D%220%200%201%201%22%3E%3CforeignObject%20width%3D%221%22%20height%3D%221%22%20requiredExtensions%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%3E%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22width%3A%201px%3B%20height%3A%201px%3B%20background%3A%20red%3B%20position%3A%20relative%22%3E%3C%2Fdiv%3E%3C%2FforeignObject%3E%3C%2Fsvg%3E",await n,e.drawImage(i,0,0),l=e.getImageData(5,5,1,1).data[3]<128}return l})().then((t=>{null==e||e(t?[u]:[])})):null==e||e([]),i)};return Object.assign(e,i)};let p,m;function u(t){const e="object"==typeof t&&t.target||document,i="object"==typeof t?t.zoom:t;window[h]||(Object.defineProperty(window,h,{configurable:!0,value:!0}),document.body.style.zoom=1.0001,document.body.offsetHeight,document.body.style.zoom=1,window.addEventListener("message",(({data:t,origin:e})=>{if(e===window.origin)try{if(t&&"string"==typeof t&&t.startsWith(c)){const[,e]=t.split(","),i=Number.parseFloat(e);Number.isNaN(i)||(m=i)}}catch(t){console.error(t)}})));let n=!1;Array.from(e.querySelectorAll("svg[data-marpit-svg]"),(t=>{var e,s,r,o;t.style.transform||(t.style.transform="translateZ(0)");const a=i||m||t.currentScale||1;p!==a&&(p=a,n=a);const l=t.getBoundingClientRect(),{length:c}=t.children;for(let i=0;i<c;i+=1){const n=t.children[i];if(n.getScreenCTM){const t=n.getScreenCTM();if(t){const i=null!==(s=null===(e=n.x)||void 0===e?void 0:e.baseVal.value)&&void 0!==s?s:0,c=null!==(o=null===(r=n.y)||void 0===r?void 0:r.baseVal.value)&&void 0!==o?o:0,d=n.children.length;for(let e=0;e<d;e+=1){const s=n.children[e];if("SECTION"===s.tagName){const{style:e}=s;e.transformOrigin||(e.transformOrigin=`${-i}px ${-c}px`),e.transform=`scale(${a}) matrix(${t.a}, ${t.b}, ${t.c}, ${t.d}, ${t.e-l.left}, ${t.f-l.top}) translateZ(0.0001px)`;break}}}}}})),!1!==n&&Array.from(e.querySelectorAll("iframe"),(({contentWindow:t})=>{null==t||t.postMessage(`${c}${n}`,"null"===window.origin?"*":window.origin)}))}function v({once:t=!1,target:e=document}={}){const i=function(t=document){if(t[d])return t[d];let e=!0;const i=()=>{e=!1,delete t[d]};Object.defineProperty(t,d,{configurable:!0,value:i});let n=[],s=!1;(async()=>{try{n=await g()}finally{s=!0}})();const r=()=>{for(const e of n)e({target:t});s&&0===n.length||e&&window.requestAnimationFrame(r)};return r(),i}(e);return t?(i(),()=>{}):i}p=1,m=void 0;const w=Symbol(),b=(e=document)=>{if("undefined"==typeof window)throw new Error("Marp Core's browser script is valid only in browser context.");if(((e=document)=>{const i=window[a];i||customElements.define("marp-auto-scaling",s);for(const n of Object.keys(t)){const s=`marp-${n}`,a=t[n].proto();(o??(o=!!document.createElement("div",{is:"marp-auto-scaling"}).outerHTML.startsWith("<div is"),o))&&a!==HTMLElement?i||customElements.define(s,r(a,{style:t[n].style}),{extends:n}):(i||customElements.define(s,r(HTMLElement,t[n])),e.querySelectorAll(`${n}[is="${s}"]`).forEach((t=>{t.outerHTML=t.outerHTML.replace(new RegExp(`^<${n}`,"i"),`<${s}`).replace(new RegExp(`</${n}>$`,"i"),`</${s}>`)})))}window[a]=!0})(e),e[w])return e[w];const i=v({target:e}),n=()=>{i(),delete e[w]},l=Object.assign(n,{cleanup:n,update:()=>b(e)});return Object.defineProperty(e,w,{configurable:!0,value:l}),l},y=document.currentScript;b(y?y.getRootNode():document)}();
</script></foreignObject></svg></div><div class="bespoke-marp-note" data-index="0" tabindex="0"><p>MARP slide deck: Transformer e GPT - Appendice Corso NLP</p></div><div class="bespoke-marp-note" data-index="12" tabindex="0"><p>Iniziamo con le importazioni necessarie e la definizione della classe di configurazione del nostro modello GPT.

Importiamo math per alcune operazioni matematiche, torch come libreria principale, nn per i moduli neurali e F per le funzioni di attivazione e altre utility.

La classe GPTConfig ci permette di definire tutti i parametri principali del nostro modello:
- vocab_size: la dimensione del vocabolario, ovvero quante parole o token diversi il modello può riconoscere
- block_size: la lunghezza massima della sequenza che il modello può elaborare
- n_embd: la dimensione degli embedding, ovvero quanto sono &quot;ricche&quot; le rappresentazioni vettoriali
- n_layer: il numero di blocchi del decoder impilati uno sull'altro
- n_head: il numero di teste di attenzione in ogni blocco
- dropout: la probabilità di dropout, una tecnica di regolarizzazione

Questi parametri determinano la capacità e le dimensioni del modello. Per riferimento, GPT-2 Small ha 12 layer, 12 teste e 768 dimensioni di embedding, mentre GPT-3 arriva a 96 layer, 96 teste e 12288 dimensioni.

Domanda tecnica: Come pensate che la scelta di questi iperparametri influenzi le capacità e le prestazioni del modello?</p></div><div class="bespoke-marp-note" data-index="13" tabindex="0"><p>Qui implementiamo la classe SelfAttention, che è il cuore del nostro modello GPT.

Iniziamo verificando che la dimensione degli embedding sia divisibile per il numero di teste, in modo da poter dividere equamente le dimensioni tra le teste.

Creiamo tre trasformazioni lineari per calcolare key, query e value a partire dall'input. Queste sono semplicemente matrici di pesi che trasformano l'input in tre diversi spazi vettoriali.

Aggiungiamo anche una proiezione di output che combinerà i risultati delle diverse teste di attenzione.

Per la regolarizzazione, utilizziamo due layer di dropout: uno per i pesi di attenzione e uno per l'output residuale.

Un elemento cruciale è la maschera causale (causal mask), che assicura che ogni posizione possa vedere solo le posizioni precedenti. Questa è implementata come una matrice triangolare inferiore di uni.

Infine, memorizziamo il numero di teste e la dimensione degli embedding per uso futuro.

Domanda per approfondire: Perché è importante che la dimensione degli embedding sia divisibile per il numero di teste? Come influisce questo sulla struttura del multi-head attention?</p></div><div class="bespoke-marp-note" data-index="14" tabindex="0"><p>Qui implementiamo il metodo forward del nostro modulo SelfAttention, che definisce come l'informazione fluisce attraverso questo componente.

Iniziamo estraendo le dimensioni dell'input: B è la dimensione del batch, T è la lunghezza della sequenza, e C è la dimensionalità degli embedding.

Calcoliamo key, query e value per tutte le teste in parallelo. Notiamo la manipolazione delle dimensioni: dividiamo C in n_head parti uguali, e riorganizziamo il tensore per separare le teste.

Poi calcoliamo i punteggi di attenzione moltiplicando q per k trasposto. Scaliamo dividendo per la radice quadrata della dimensione delle key, come discusso in precedenza.

Applichiamo la maschera causale, sostituendo con -infinito i valori che corrispondono a posizioni future, in modo che dopo il softmax diventino zero.

Applichiamo softmax per normalizzare i punteggi in pesi di attenzione, seguita da dropout per regolarizzazione.

Moltiplichiamo i pesi per i valori per ottenere l'output pesato, e riorganizziamo il tensore per combinare i risultati di tutte le teste.

Infine, applichiamo una proiezione lineare e dropout all'output.

Domanda tecnica: Cosa succede esattamente quando sostituiamo con -infinito i valori mascherati? Perché questo è necessario per implementare l'attenzione causale?</p></div><div class="bespoke-marp-note" data-index="15" tabindex="0"><p>Qui implementiamo il modulo FeedForward, che è la rete feed-forward applicata dopo ogni blocco di attenzione.

La struttura è semplice: una rete sequenziale con due trasformazioni lineari e un'attivazione non lineare nel mezzo.

La prima trasformazione espande la dimensionalità da n_embd a 4*n_embd, introducendo più parametri e capacità rappresentativa.

Utilizziamo l'attivazione GELU (Gaussian Error Linear Unit) invece di ReLU. GELU è stata introdotta più recentemente e ha dimostrato di funzionare meglio in molti modelli di linguaggio, inclusi i modelli GPT.

La seconda trasformazione riporta la dimensionalità a n_embd, e infine applichiamo dropout per regolarizzazione.

Questa struttura a &quot;collo di bottiglia&quot; (espansione seguita da compressione) è comune in molte architetture di deep learning e permette alla rete di apprendere trasformazioni complesse.

Domanda per la classe: Perché pensate che si utilizzi un fattore di espansione di 4 nella prima trasformazione lineare? Come potrebbe influire questo sulla capacità del modello?</p></div><div class="bespoke-marp-note" data-index="16" tabindex="0"><p>Qui combiniamo self-attention e feed-forward in un blocco completo del Transformer.

Ogni blocco contiene:
- Un layer di normalizzazione seguito da self-attention
- Un altro layer di normalizzazione seguito da feed-forward
- Connessioni residuali attorno a entrambi i sub-layer

Notiamo che utilizziamo la normalizzazione pre-layer (applicando layer norm prima di ogni sub-layer) invece della normalizzazione post-layer utilizzata nel paper originale. Questa è una modifica introdotta in GPT-2 che ha dimostrato di migliorare la stabilità dell'addestramento.

Le connessioni residuali (x + ...) sono cruciali per l'addestramento di reti profonde, permettendo ai gradienti di fluire più facilmente attraverso la rete.

Questo blocco è l'unità fondamentale che verrà ripetuta più volte per formare il modello completo. Nei modelli GPT moderni, possono esserci decine o centinaia di questi blocchi impilati.

Domanda per stimolare la riflessione: Perché pensate che la normalizzazione pre-layer funzioni meglio della normalizzazione post-layer nei modelli profondi? Quali vantaggi potrebbe offrire durante l'addestramento?</p></div><div class="bespoke-marp-note" data-index="17" tabindex="0"><p>Qui iniziamo l'implementazione del modello GPT completo.

Iniziamo definendo gli embedding:
- tok_emb: l'embedding dei token, che mappa ogni token del vocabolario a un vettore di dimensione n_embd
- pos_emb: l'embedding posizionale, che viene appreso durante l'addestramento invece di utilizzare le funzioni sinusoidali fisse del paper originale
- drop: un layer di dropout applicato dopo la somma degli embedding

Poi definiamo i blocchi del transformer, creando n_layer istanze del blocco che abbiamo definito prima e combinandole in un modulo Sequential.

Aggiungiamo un layer di normalizzazione finale dopo tutti i blocchi.

Infine, definiamo la &quot;testa&quot; di language modeling, che è semplicemente una trasformazione lineare che mappa le rappresentazioni finali alle probabilità sul vocabolario.

Notiamo anche che includiamo un metodo per inizializzare i pesi del modello, che verrà applicato a tutti i sotto-moduli.

Domanda per la classe: Perché pensate che in GPT si utilizzi un embedding posizionale appreso invece delle funzioni sinusoidali fisse proposte nel paper originale? Quali vantaggi e svantaggi potrebbe avere questo approccio?</p></div><div class="bespoke-marp-note" data-index="18" tabindex="0"><p>Continuiamo con l'implementazione del modello GPT, definendo il metodo di inizializzazione dei pesi e il forward pass.

Il metodo _init_weights inizializza i pesi delle trasformazioni lineari e degli embedding con una distribuzione normale con media 0 e deviazione standard 0.02, mentre i bias vengono inizializzati a zero. Per i layer di normalizzazione, i pesi vengono inizializzati a 1 e i bias a 0.

Nel metodo forward, prima verifichiamo che la lunghezza della sequenza non superi il block_size configurato.

Poi calcoliamo gli embedding dei token e gli embedding posizionali, li sommiamo e applichiamo dropout.

Facciamo passare il risultato attraverso tutti i blocchi del transformer, seguiti dal layer di normalizzazione finale.

Infine, applichiamo la testa di language modeling per ottenere i logits, che rappresentano le probabilità non normalizzate per ogni token del vocabolario.

Se vengono forniti dei target, calcoliamo anche la loss utilizzando la cross-entropy, che è la funzione di perdita standard per i problemi di classificazione multi-classe come il language modeling.

Domanda tecnica: Perché utilizziamo la cross-entropy come funzione di perdita per il language modeling? Come si collega questa scelta alla natura probabilistica del task?</p></div><div class="bespoke-marp-note" data-index="19" tabindex="0"><p>Qui implementiamo il metodo generate, che ci permette di utilizzare il modello per generare nuovo testo in modo autoregressive.

Il metodo prende in input:
- idx: un tensore di indici che rappresenta il contesto iniziale
- max_new_tokens: il numero di nuovi token da generare
- temperature: un parametro che controlla la &quot;creatività&quot; della generazione (valori più alti aumentano la diversità)
- top_k: se specificato, limita il campionamento ai k token più probabili

Il processo di generazione è iterativo:
1. Se il contesto è troppo lungo, lo tagliamo per rispettare il block_size
2. Facciamo un forward pass per ottenere le probabilità di tutti i token
3. Ci concentriamo sull'ultimo time step, che rappresenta la predizione per il token successivo
4. Applichiamo la temperature scaling per controllare la diversità
5. Se specificato, limitiamo le probabilità ai top_k token
6. Convertiamo i logits in probabilità con softmax
7. Campioniamo il prossimo token dalla distribuzione di probabilità
8. Aggiungiamo il token generato alla sequenza e ripetiamo

Questo approccio autoregressive è il cuore dei modelli generativi come GPT: ogni token generato diventa parte del contesto per generare il token successivo.

Domanda per stimolare la discussione: Come pensate che i parametri temperature e top_k influenzino la qualità e la diversità del testo generato? Quali valori usereste per diverse applicazioni?</p></div><div class="bespoke-marp-note" data-index="20" tabindex="0"><p>Qui mostriamo un esempio di come utilizzare il nostro modello GPT implementato.

Iniziamo definendo una configurazione per un modello relativamente piccolo, simile a GPT-2 Small, con:
- Un vocabolario di 50257 token (come GPT-2)
- Una lunghezza massima di sequenza di 1024 token
- Embedding di dimensione 768
- 12 blocchi del transformer
- 12 teste di attenzione per blocco
- Un dropout di 0.1 per regolarizzazione

Poi inizializziamo il modello con questa configurazione.

Per testare il forward pass, creiamo un batch di input e target casuali e li passiamo al modello, ottenendo i logits e la loss.

Infine, mostriamo come generare nuovo testo a partire da un contesto iniziale, specificando:
- Il numero di nuovi token da generare (20)
- La temperatura (0.8, leggermente più creativa del default)
- Il parametro top_k (40, come in molte implementazioni di GPT-2)

Questo esempio mostra l'intero flusso di utilizzo del modello, dall'inizializzazione alla generazione di testo.

Domanda per la classe: Quali applicazioni pratiche potreste immaginare per un modello GPT come quello che abbiamo implementato? Come potrebbe essere adattato per task specifici?</p></div><div class="bespoke-marp-note" data-index="21" tabindex="0"><p>Qui implementiamo una semplice funzione di addestramento per il nostro modello GPT.

La funzione prende in input:
- Il modello da addestrare
- Un data loader che fornisce batch di dati
- Un ottimizzatore (tipicamente Adam o AdamW)
- Il numero di epoche di addestramento
- Il dispositivo su cui eseguire l'addestramento (CPU o GPU)

Il processo di addestramento è standard:
1. Mettiamo il modello in modalità di addestramento
2. Per ogni epoca, iteriamo sui batch di dati
3. Spostiamo i dati sul dispositivo appropriato
4. Azzeriamo i gradienti dell'ottimizzatore
5. Facciamo un forward pass per ottenere i logits e la loss
6. Facciamo un backward pass per calcolare i gradienti
7. Aggiorniamo i pesi con l'ottimizzatore
8. Teniamo traccia della loss e stampiamo aggiornamenti periodici

Questo è un loop di addestramento molto basilare. In pratica, l'addestramento di modelli GPT di grandi dimensioni richiede molte ottimizzazioni aggiuntive come mixed precision training, gradient accumulation, learning rate scheduling, e altro ancora.

Domanda per approfondire: Quali sfide pensate si incontrino nell'addestrare modelli GPT di grandi dimensioni? Come si potrebbero affrontare queste sfide?</p></div><div class="bespoke-marp-note" data-index="22" tabindex="0"><p>Nella pratica, l'implementazione di un modello GPT completo richiederebbe diverse ottimizzazioni e considerazioni aggiuntive.

Per l'efficienza computazionale, tecniche come mixed precision training (utilizzando float16 invece di float32), gradient accumulation (accumulando gradienti su più batch prima di aggiornare i pesi), e model parallelism (distribuendo il modello su più GPU) sono essenziali per addestrare modelli di grandi dimensioni.

La tokenizzazione è un aspetto cruciale. Modelli come GPT-2 e GPT-3 utilizzano Byte-Pair Encoding (BPE), che bilancia efficacemente la rappresentazione di parole comuni e rare, creando un vocabolario di subword tokens.

Per quanto riguarda le strategie di addestramento, tecniche come learning rate scheduling (variare il learning rate durante l'addestramento), weight decay (regolarizzazione L2), e gradient clipping (limitare la norma dei gradienti) sono importanti per la stabilità dell'addestramento.

Infine, per la generazione di testo, strategie avanzate come beam search (mantenere multiple ipotesi di generazione), nucleus sampling (campionare dalla distribuzione troncata ai token più probabili), e temperature scaling (controllare la diversità) possono migliorare significativamente la qualità del testo generato.

Domanda per stimolare la riflessione: Quali di queste ottimizzazioni pensate siano più critiche per il successo pratico dei modelli GPT? Perché?</p></div><div class="bespoke-marp-note" data-index="23" tabindex="0"><p>Per completare la nostra comprensione, è utile confrontare le architetture BERT e GPT, che rappresentano due approcci fondamentali ai modelli Transformer.

BERT utilizza principalmente la parte encoder del Transformer e considera il contesto in entrambe le direzioni (precedente e successivo). Durante il pre-addestramento, utilizza Masked Language Modeling (MLM), dove alcune parole vengono mascherate e il modello deve predirle, e Next Sentence Prediction (NSP). BERT eccelle in compiti di comprensione del linguaggio.

GPT utilizza principalmente la parte decoder del Transformer e considera solo il contesto precedente (left-to-right). Durante il pre-addestramento, predice semplicemente la parola successiva (language modeling classico). GPT eccelle in compiti generativi.

Le principali differenze possono essere riassunte così:
1. Direzione del contesto: BERT è bidirezionale, GPT è unidirezionale
2. Obiettivo di pre-addestramento: BERT predice parole mascherate, GPT predice la parola successiva
3. Uso tipico: BERT per comprensione, GPT per generazione
4. Architettura: BERT usa l'encoder, GPT usa il decoder

Entrambi gli approcci hanno i loro punti di forza e sono complementari in molte applicazioni NLP.

Domanda per la classe: In quali scenari applicativi pensate sia più appropriato utilizzare BERT rispetto a GPT, e viceversa? Ci sono casi in cui potrebbero essere utilizzati insieme?</p></div><script>/*!! License: https://unpkg.com/@marp-team/marp-cli@4.1.2/lib/bespoke.js.LICENSE.txt */
!function(){"use strict";function e(e){return e&&e.__esModule&&Object.prototype.hasOwnProperty.call(e,"default")?e.default:e}var t,n,r=(n||(n=1,t={from:function(e,t){var n,r=1===(e.parent||e).nodeType?e.parent||e:document.querySelector(e.parent||e),o=[].filter.call("string"==typeof e.slides?r.querySelectorAll(e.slides):e.slides||r.children,(function(e){return"SCRIPT"!==e.nodeName})),a={},i=function(e,t){return(t=t||{}).index=o.indexOf(e),t.slide=e,t},s=function(e,t){a[e]=(a[e]||[]).filter((function(e){return e!==t}))},c=function(e,t){return(a[e]||[]).reduce((function(e,n){return e&&!1!==n(t)}),!0)},l=function(e,t){o[e]&&(n&&c("deactivate",i(n,t)),n=o[e],c("activate",i(n,t)))},d=function(e,t){var r=o.indexOf(n)+e;c(e>0?"next":"prev",i(n,t))&&l(r,t)},u={off:s,on:function(e,t){return(a[e]||(a[e]=[])).push(t),s.bind(null,e,t)},fire:c,slide:function(e,t){if(!arguments.length)return o.indexOf(n);c("slide",i(o[e],t))&&l(e,t)},next:d.bind(null,1),prev:d.bind(null,-1),parent:r,slides:o,destroy:function(e){c("destroy",i(n,e)),a={}}};return(t||[]).forEach((function(e){e(u)})),n||l(0),u}}),t),o=e(r);const a=document.body,i=(...e)=>history.replaceState(...e),s="",c="presenter",l="next",d=["",c,l],u="bespoke-marp-",f=`data-${u}`,m=(e,{protocol:t,host:n,pathname:r,hash:o}=location)=>{const a=e.toString();return`${t}//${n}${r}${a?"?":""}${a}${o}`},g=()=>a.dataset.bespokeView,p=e=>new URLSearchParams(location.search).get(e),v=(e,t={})=>{const n={location,setter:i,...t},r=new URLSearchParams(n.location.search);for(const t of Object.keys(e)){const n=e[t];"string"==typeof n?r.set(t,n):r.delete(t)}try{n.setter({...window.history.state??{}},"",m(r,n.location))}catch(e){console.error(e)}},h=(()=>{const e="bespoke-marp";try{return localStorage.setItem(e,e),localStorage.removeItem(e),!0}catch{return!1}})(),y=e=>{try{return localStorage.getItem(e)}catch{return null}},b=(e,t)=>{try{return localStorage.setItem(e,t),!0}catch{return!1}},w=e=>{try{return localStorage.removeItem(e),!0}catch{return!1}},x=(e,t)=>{const n="aria-hidden";t?e.setAttribute(n,"true"):e.removeAttribute(n)},k=e=>{e.parent.classList.add(`${u}parent`),e.slides.forEach((e=>e.classList.add(`${u}slide`))),e.on("activate",(t=>{const n=`${u}active`,r=t.slide,o=r.classList,a=!o.contains(n);if(e.slides.forEach((e=>{e.classList.remove(n),x(e,!0)})),o.add(n),x(r,!1),a){const e=`${n}-ready`;o.add(e),document.body.clientHeight,o.remove(e)}}))},$=e=>{let t=0,n=0;Object.defineProperty(e,"fragments",{enumerable:!0,value:e.slides.map((e=>[null,...e.querySelectorAll("[data-marpit-fragment]")]))});const r=r=>void 0!==e.fragments[t][n+r],o=(r,o)=>{t=r,n=o,e.fragments.forEach(((e,t)=>{e.forEach(((e,n)=>{if(null==e)return;const a=t<r||t===r&&n<=o;e.setAttribute(`${f}fragment`,(a?"":"in")+"active");const i=`${f}current-fragment`;t===r&&n===o?e.setAttribute(i,"current"):e.removeAttribute(i)}))})),e.fragmentIndex=o;const a={slide:e.slides[r],index:r,fragments:e.fragments[r],fragmentIndex:o};e.fire("fragment",a)};e.on("next",(({fragment:a=!0})=>{if(a){if(r(1))return o(t,n+1),!1;const a=t+1;e.fragments[a]&&o(a,0)}else{const r=e.fragments[t].length;if(n+1<r)return o(t,r-1),!1;const a=e.fragments[t+1];a&&o(t+1,a.length-1)}})),e.on("prev",(({fragment:a=!0})=>{if(r(-1)&&a)return o(t,n-1),!1;const i=t-1;e.fragments[i]&&o(i,e.fragments[i].length-1)})),e.on("slide",(({index:t,fragment:n})=>{let r=0;if(void 0!==n){const o=e.fragments[t];if(o){const{length:e}=o;r=-1===n?e-1:Math.min(Math.max(n,0),e-1)}}o(t,r)})),o(0,0)},E=document,L=()=>!(!E.fullscreenEnabled&&!E.webkitFullscreenEnabled),S=()=>!(!E.fullscreenElement&&!E.webkitFullscreenElement),P=e=>{e.fullscreen=()=>{L()&&(async()=>{S()?(E.exitFullscreen||E.webkitExitFullscreen)?.call(E):((e=E.body)=>{(e.requestFullscreen||e.webkitRequestFullscreen)?.call(e)})()})()},document.addEventListener("keydown",(t=>{"f"!==t.key&&"F11"!==t.key||t.altKey||t.ctrlKey||t.metaKey||!L()||(e.fullscreen(),t.preventDefault())}))},_=`${u}inactive`,T=(e=2e3)=>({parent:t,fire:n})=>{const r=t.classList,o=e=>n(`marp-${e?"":"in"}active`);let a;const i=()=>{a&&clearTimeout(a),a=setTimeout((()=>{r.add(_),o()}),e),r.contains(_)&&(r.remove(_),o(!0))};for(const e of["mousedown","mousemove","touchend"])document.addEventListener(e,i);setTimeout(i,0)},I=["AUDIO","BUTTON","INPUT","SELECT","TEXTAREA","VIDEO"],M=e=>{e.parent.addEventListener("keydown",(e=>{if(!e.target)return;const t=e.target;(I.includes(t.nodeName)||"true"===t.contentEditable)&&e.stopPropagation()}))},O=e=>{window.addEventListener("load",(()=>{for(const t of e.slides){const e=t.querySelector("marp-auto-scaling, [data-auto-scaling], [data-marp-fitting]");t.setAttribute(`${f}load`,e?"":"hideable")}}))},A=({interval:e=250}={})=>t=>{document.addEventListener("keydown",(e=>{if(" "===e.key&&e.shiftKey)t.prev();else if("ArrowLeft"===e.key||"ArrowUp"===e.key||"PageUp"===e.key)t.prev({fragment:!e.shiftKey});else if(" "!==e.key||e.shiftKey)if("ArrowRight"===e.key||"ArrowDown"===e.key||"PageDown"===e.key)t.next({fragment:!e.shiftKey});else if("End"===e.key)t.slide(t.slides.length-1,{fragment:-1});else{if("Home"!==e.key)return;t.slide(0)}else t.next();e.preventDefault()}));let n,r,o=0;t.parent.addEventListener("wheel",(a=>{let i=!1;const s=(e,t)=>{e&&(i=i||((e,t)=>((e,t)=>{const n="X"===t?"Width":"Height";return e[`client${n}`]<e[`scroll${n}`]})(e,t)&&((e,t)=>{const{overflow:n}=e,r=e[`overflow${t}`];return"auto"===n||"scroll"===n||"auto"===r||"scroll"===r})(getComputedStyle(e),t))(e,t)),e?.parentElement&&s(e.parentElement,t)};if(0!==a.deltaX&&s(a.target,"X"),0!==a.deltaY&&s(a.target,"Y"),i)return;a.preventDefault();const c=Math.sqrt(a.deltaX**2+a.deltaY**2);if(void 0!==a.wheelDelta){if(void 0===a.webkitForce&&Math.abs(a.wheelDelta)<40)return;if(a.deltaMode===a.DOM_DELTA_PIXEL&&c<4)return}else if(a.deltaMode===a.DOM_DELTA_PIXEL&&c<12)return;r&&clearTimeout(r),r=setTimeout((()=>{n=0}),e);const l=Date.now()-o<e,d=c<=n;if(n=c,l||d)return;let u;(a.deltaX>0||a.deltaY>0)&&(u="next"),(a.deltaX<0||a.deltaY<0)&&(u="prev"),u&&(t[u](),o=Date.now())}))},C=(e=`.${u}osc`)=>{const t=document.querySelector(e);if(!t)return()=>{};const n=(e,n)=>{t.querySelectorAll(`[${f}osc=${JSON.stringify(e)}]`).forEach(n)};return L()||n("fullscreen",(e=>e.style.display="none")),h||n("presenter",(e=>{e.disabled=!0,e.title="Presenter view is disabled due to restricted localStorage."})),e=>{t.addEventListener("click",(t=>{if(t.target instanceof HTMLElement){const{bespokeMarpOsc:n}=t.target.dataset;n&&t.target.blur();const r={fragment:!t.shiftKey};"next"===n?e.next(r):"prev"===n?e.prev(r):"fullscreen"===n?e?.fullscreen():"presenter"===n&&e.openPresenterView()}})),e.parent.appendChild(t),e.on("activate",(({index:t})=>{n("page",(n=>n.textContent=`Page ${t+1} of ${e.slides.length}`))})),e.on("fragment",(({index:t,fragments:r,fragmentIndex:o})=>{n("prev",(e=>e.disabled=0===t&&0===o)),n("next",(n=>n.disabled=t===e.slides.length-1&&o===r.length-1))})),e.on("marp-active",(()=>x(t,!1))),e.on("marp-inactive",(()=>x(t,!0))),L()&&(e=>{for(const t of["","webkit"])E.addEventListener(t+"fullscreenchange",e)})((()=>n("fullscreen",(e=>e.classList.toggle("exit",L()&&S())))))}},D=e=>{window.addEventListener("message",(t=>{if(t.origin!==window.origin)return;const[n,r]=t.data.split(":");if("navigate"===n){const[t,n]=r.split(",");let o=Number.parseInt(t,10),a=Number.parseInt(n,10)+1;a>=e.fragments[o].length&&(o+=1,a=0),e.slide(o,{fragment:a})}}))};var N,B,q,K,j,F,V,U={exports:{}},X=(N||(N=1,U.exports=(B=["area","base","br","col","command","embed","hr","img","input","keygen","link","meta","param","source","track","wbr"],q=function(e){return String(e).replace(/[&<>"']/g,(function(e){return"&"+K[e]+";"}))},K={"&":"amp","<":"lt",">":"gt",'"':"quot","'":"apos"},j="dangerouslySetInnerHTML",F={className:"class",htmlFor:"for"},V={},function(e,t){var n=[],r="";t=t||{};for(var o=arguments.length;o-- >2;)n.push(arguments[o]);if("function"==typeof e)return t.children=n.reverse(),e(t);if(e){if(r+="<"+e,t)for(var a in t)!1!==t[a]&&null!=t[a]&&a!==j&&(r+=" "+(F[a]?F[a]:q(a))+'="'+q(t[a])+'"');r+=">"}if(-1===B.indexOf(e)){if(t[j])r+=t[j].__html;else for(;n.length;){var i=n.pop();if(i)if(i.pop)for(var s=i.length;s--;)n.push(i[s]);else r+=!0===V[i]?i:q(i)}r+=e?"</"+e+">":""}return V[r]=!0,r})),U.exports),H=e(X);const R=({children:e})=>H(null,null,...e),W=`${u}presenter-`,J={container:`${W}container`,dragbar:`${W}dragbar-container`,next:`${W}next`,nextContainer:`${W}next-container`,noteContainer:`${W}note-container`,noteWrapper:`${W}note-wrapper`,noteButtons:`${W}note-buttons`,infoContainer:`${W}info-container`,infoPage:`${W}info-page`,infoPageText:`${W}info-page-text`,infoPagePrev:`${W}info-page-prev`,infoPageNext:`${W}info-page-next`,noteButtonsBigger:`${W}note-bigger`,noteButtonsSmaller:`${W}note-smaller`,infoTime:`${W}info-time`,infoTimer:`${W}info-timer`},Y=e=>{const{title:t}=document;document.title="[Presenter view]"+(t?` - ${t}`:"");const n={},r=e=>(n[e]=n[e]||document.querySelector(`.${e}`),n[e]);document.body.appendChild((e=>{const t=document.createElement("div");return t.className=J.container,t.appendChild(e),t.insertAdjacentHTML("beforeend",H(R,null,H("div",{class:J.nextContainer},H("iframe",{class:J.next,src:"?view=next"})),H("div",{class:J.dragbar}),H("div",{class:J.noteContainer},H("div",{class:J.noteWrapper}),H("div",{class:J.noteButtons},H("button",{class:J.noteButtonsSmaller,tabindex:"-1",title:"Smaller notes font size"},"Smaller notes font size"),H("button",{class:J.noteButtonsBigger,tabindex:"-1",title:"Bigger notes font size"},"Bigger notes font size"))),H("div",{class:J.infoContainer},H("div",{class:J.infoPage},H("button",{class:J.infoPagePrev,tabindex:"-1",title:"Previous"},"Previous"),H("span",{class:J.infoPageText}),H("button",{class:J.infoPageNext,tabindex:"-1",title:"Next"},"Next")),H("time",{class:J.infoTime,title:"Current time"}),H("time",{class:J.infoTimer,title:"Timer"})))),t})(e.parent)),(e=>{let t=!1;r(J.dragbar).addEventListener("mousedown",(()=>{t=!0,r(J.dragbar).classList.add("active")})),window.addEventListener("mouseup",(()=>{t=!1,r(J.dragbar).classList.remove("active")})),window.addEventListener("mousemove",(e=>{if(!t)return;const n=e.clientX/document.documentElement.clientWidth*100;r(J.container).style.setProperty("--bespoke-marp-presenter-split-ratio",`${Math.max(0,Math.min(100,n))}%`)})),r(J.nextContainer).addEventListener("click",(()=>e.next()));const n=r(J.next),o=(a=n,(e,t)=>a.contentWindow?.postMessage(`navigate:${e},${t}`,"null"===window.origin?"*":window.origin));var a;n.addEventListener("load",(()=>{r(J.nextContainer).classList.add("active"),o(e.slide(),e.fragmentIndex),e.on("fragment",(({index:e,fragmentIndex:t})=>o(e,t)))}));const i=document.querySelectorAll(".bespoke-marp-note");i.forEach((e=>{e.addEventListener("keydown",(e=>e.stopPropagation())),r(J.noteWrapper).appendChild(e)})),e.on("activate",(()=>i.forEach((t=>t.classList.toggle("active",t.dataset.index==e.slide())))));let s=0;const c=e=>{s=Math.max(-5,s+e),r(J.noteContainer).style.setProperty("--bespoke-marp-note-font-scale",(1.2**s).toFixed(4))},l=()=>c(1),d=()=>c(-1),u=r(J.noteButtonsBigger),f=r(J.noteButtonsSmaller);u.addEventListener("click",(()=>{u.blur(),l()})),f.addEventListener("click",(()=>{f.blur(),d()})),document.addEventListener("keydown",(e=>{"+"===e.key&&l(),"-"===e.key&&d()}),!0),e.on("activate",(({index:t})=>{r(J.infoPageText).textContent=`${t+1} / ${e.slides.length}`}));const m=r(J.infoPagePrev),g=r(J.infoPageNext);m.addEventListener("click",(t=>{m.blur(),e.prev({fragment:!t.shiftKey})})),g.addEventListener("click",(t=>{g.blur(),e.next({fragment:!t.shiftKey})})),e.on("fragment",(({index:t,fragments:n,fragmentIndex:r})=>{m.disabled=0===t&&0===r,g.disabled=t===e.slides.length-1&&r===n.length-1}));let p=new Date;const v=()=>{const e=new Date,t=e=>`${Math.floor(e)}`.padStart(2,"0"),n=e.getTime()-p.getTime(),o=t(n/1e3%60),a=t(n/1e3/60%60),i=t(n/36e5%24);r(J.infoTime).textContent=e.toLocaleTimeString(),r(J.infoTimer).textContent=`${i}:${a}:${o}`};v(),setInterval(v,250),r(J.infoTimer).addEventListener("click",(()=>{p=new Date}))})(e)},z=e=>{if(!(e=>e.syncKey&&"string"==typeof e.syncKey)(e))throw new Error("The current instance of Bespoke.js is invalid for Marp bespoke presenter plugin.");Object.defineProperties(e,{openPresenterView:{enumerable:!0,value:G},presenterUrl:{enumerable:!0,get:Q}}),h&&document.addEventListener("keydown",(t=>{"p"!==t.key||t.altKey||t.ctrlKey||t.metaKey||(t.preventDefault(),e.openPresenterView())}))};function G(){const{max:e,floor:t}=Math,n=e(t(.85*window.innerWidth),640),r=e(t(.85*window.innerHeight),360);return window.open(this.presenterUrl,W+this.syncKey,`width=${n},height=${r},menubar=no,toolbar=no`)}function Q(){const e=new URLSearchParams(location.search);return e.set("view","presenter"),e.set("sync",this.syncKey),m(e)}const Z=e=>{const t=g();return t===l&&e.appendChild(document.createElement("span")),{[s]:z,[c]:Y,[l]:D}[t]},ee=e=>{e.on("activate",(t=>{document.querySelectorAll(".bespoke-progress-parent > .bespoke-progress-bar").forEach((n=>{n.style.flexBasis=100*t.index/(e.slides.length-1)+"%"}))}))},te=e=>{const t=Number.parseInt(e,10);return Number.isNaN(t)?null:t},ne=(e={})=>{const t={history:!0,...e};return e=>{let n=!0;const r=e=>{const t=n;try{return n=!0,e()}finally{n=t}},o=(t={fragment:!0})=>{let n=t.fragment?te(p("f")||""):null;((t,n)=>{const{min:r,max:o}=Math,{fragments:a,slides:i}=e,s=o(0,r(t,i.length-1)),c=o(0,r(n||0,a[s].length-1));s===e.slide()&&c===e.fragmentIndex||e.slide(s,{fragment:c})})((()=>{if(location.hash){const[t]=location.hash.slice(1).split(":~:");if(/^\d+$/.test(t))return(te(t)??1)-1;const r=document.getElementById(t)||document.querySelector(`a[name="${CSS.escape(t)}"]`);if(r){const{length:t}=e.slides;for(let o=0;o<t;o+=1)if(e.slides[o].contains(r)){const t=e.fragments?.[o],a=r.closest("[data-marpit-fragment]");if(t&&a){const e=t.indexOf(a);e>=0&&(n=e)}return o}}}return 0})(),n)};e.on("fragment",(({index:e,fragmentIndex:r})=>{n||v({f:0===r||r.toString()},{location:{...location,hash:`#${e+1}`},setter:(...e)=>t.history?history.pushState(...e):history.replaceState(...e)})})),setTimeout((()=>{o(),window.addEventListener("hashchange",(()=>r((()=>{o({fragment:!1}),v({f:void 0})})))),window.addEventListener("popstate",(()=>{n||r((()=>o()))})),n=!1}),0)}},re=(e={})=>{const t=e.key||window.history.state?.marpBespokeSyncKey||Math.random().toString(36).slice(2),n=`bespoke-marp-sync-${t}`;var r;r={marpBespokeSyncKey:t},v({},{setter:(e,...t)=>i({...e,...r},...t)});const o=()=>{const e=y(n);return e?JSON.parse(e):Object.create(null)},a=e=>{const t=o(),r={...t,...e(t)};return b(n,JSON.stringify(r)),r},s=()=>{window.removeEventListener("pageshow",s),a((e=>({reference:(e.reference||0)+1})))};return e=>{s(),Object.defineProperty(e,"syncKey",{value:t,enumerable:!0});let r=!0;setTimeout((()=>{e.on("fragment",(e=>{r&&a((()=>({index:e.index,fragmentIndex:e.fragmentIndex})))}))}),0),window.addEventListener("storage",(t=>{if(t.key===n&&t.oldValue&&t.newValue){const n=JSON.parse(t.oldValue),o=JSON.parse(t.newValue);if(n.index!==o.index||n.fragmentIndex!==o.fragmentIndex)try{r=!1,e.slide(o.index,{fragment:o.fragmentIndex,forSync:!0})}finally{r=!0}}}));const i=()=>{const{reference:e}=o();void 0===e||e<=1?w(n):a((()=>({reference:e-1})))};window.addEventListener("pagehide",(e=>{e.persisted&&window.addEventListener("pageshow",s),i()})),e.on("destroy",i)}},{PI:oe,abs:ae,sqrt:ie,atan2:se}=Math,ce={passive:!0},le=({slope:e=-.7,swipeThreshold:t=30}={})=>n=>{let r;const o=n.parent,a=e=>{const t=o.getBoundingClientRect();return{x:e.pageX-(t.left+t.right)/2,y:e.pageY-(t.top+t.bottom)/2}};o.addEventListener("touchstart",(({touches:e})=>{r=1===e.length?a(e[0]):void 0}),ce),o.addEventListener("touchmove",(e=>{if(r)if(1===e.touches.length){e.preventDefault();const t=a(e.touches[0]),n=t.x-r.x,o=t.y-r.y;r.delta=ie(ae(n)**2+ae(o)**2),r.radian=se(n,o)}else r=void 0})),o.addEventListener("touchend",(o=>{if(r){if(r.delta&&r.delta>=t&&r.radian){const t=(r.radian-e+oe)%(2*oe)-oe;n[t<0?"next":"prev"](),o.stopPropagation()}r=void 0}}),ce)},de=new Map;de.clear(),de.set("none",{backward:{both:void 0,incoming:void 0,outgoing:void 0},forward:{both:void 0,incoming:void 0,outgoing:void 0}});const ue={both:"",outgoing:"outgoing-",incoming:"incoming-"},fe={forward:"",backward:"-backward"},me=e=>`--marp-bespoke-transition-animation-${e}`,ge=e=>`--marp-transition-${e}`,pe=me("name"),ve=me("duration"),he=e=>new Promise((t=>{const n={},r=document.createElement("div"),o=e=>{r.remove(),t(e)};r.addEventListener("animationstart",(()=>o(n))),Object.assign(r.style,{animationName:e,animationDuration:"1s",animationFillMode:"both",animationPlayState:"paused",position:"absolute",pointerEvents:"none"}),document.body.appendChild(r);const a=getComputedStyle(r).getPropertyValue(ge("duration"));a&&(n.defaultDuration=a),((e,t)=>{requestAnimationFrame((()=>{e.style.animationPlayState="running",requestAnimationFrame((()=>t(void 0)))}))})(r,o)})),ye=async e=>de.has(e)?de.get(e):(e=>{const t={},n=[];for(const[r,o]of Object.entries(ue))for(const[a,i]of Object.entries(fe)){const s=`marp-${o}transition${i}-${e}`;n.push(he(s).then((e=>{t[a]=t[a]||{},t[a][r]=e?{...e,name:s}:void 0})))}return Promise.all(n).then((()=>t))})(e).then((t=>(de.set(e,t),t))),be=e=>Object.values(e).flatMap(Object.values).every((e=>!e)),we=(e,{type:t,backward:n})=>{const r=e[n?"backward":"forward"],o=(()=>{const e=r[t],n=e=>({[pe]:e.name});if(e)return n(e);if(r.both){const e=n(r.both);return"incoming"===t&&(e[me("direction")]="reverse"),e}})();return!o&&n?we(e,{type:t,backward:!1}):o||{[pe]:"__bespoke_marp_transition_no_animation__"}},xe=e=>{if(e)try{const t=JSON.parse(e);if((e=>{if("object"!=typeof e)return!1;const t=e;return"string"==typeof t.name&&(void 0===t.duration||"string"==typeof t.duration)})(t))return t}catch{}},ke="_tSId",$e="_tA",Ee="bespoke-marp-transition-warming-up",Le=window.matchMedia("(prefers-reduced-motion: reduce)"),Se="__bespoke_marp_transition_reduced_outgoing__",Pe="__bespoke_marp_transition_reduced_incoming__",_e={forward:{both:void 0,incoming:{name:Pe},outgoing:{name:Se}},backward:{both:void 0,incoming:{name:Pe},outgoing:{name:Se}}},Te=e=>{if(!document.startViewTransition)return;const t=t=>(void 0!==t&&(e._tD=t),e._tD);let n;t(!1),((...e)=>{const t=[...new Set(e).values()];return Promise.all(t.map((e=>ye(e)))).then()})(...Array.from(document.querySelectorAll("section[data-transition], section[data-transition-back]")).flatMap((e=>[e.dataset.transition,e.dataset.transitionBack].flatMap((e=>{const t=xe(e);return[t?.name,t?.builtinFallback?`__builtin__${t.name}`:void 0]})).filter((e=>!!e))))).then((()=>{document.querySelectorAll("style").forEach((e=>{e.innerHTML=e.innerHTML.replace(/--marp-transition-duration:[^;}]*[;}]/g,(e=>e.slice(0,-1)+"!important"+e.slice(-1)))}))}));const r=(n,{back:r,cond:o})=>a=>{const i=t();if(i)return!!a[$e]||!("object"!=typeof i||(i.skipTransition(),!a.forSync));if(!o(a))return!0;const s=e.slides[e.slide()],c=()=>a.back??r,l="data-transition"+(c()?"-back":""),d=s.querySelector(`section[${l}]`);if(!d)return!0;const u=xe(d.getAttribute(l)??void 0);return!u||((async(e,{builtinFallback:t=!0}={})=>{let n=await ye(e);if(be(n)){if(!t)return;return n=await ye(`__builtin__${e}`),be(n)?void 0:n}return n})(u.name,{builtinFallback:u.builtinFallback}).then((e=>{if(!e){t(!0);try{n(a)}finally{t(!1)}return}let r=e;Le.matches&&(console.warn("Use a constant animation to transition because preferring reduced motion by viewer has detected."),r=_e);const o=document.getElementById(ke);o&&o.remove();const i=document.createElement("style");i.id=ke,document.head.appendChild(i),((e,t)=>{const n=[`:root{${ge("direction")}:${t.backward?-1:1};}`,":root:has(.bespoke-marp-inactive){cursor:none;}"],r=t=>{const n=e[t].both?.defaultDuration||e[t].outgoing?.defaultDuration||e[t].incoming?.defaultDuration;return"forward"===t?n:n||r("forward")},o=t.duration||r(t.backward?"backward":"forward");void 0!==o&&n.push(`::view-transition-group(*){${ve}:${o};}`);const a=e=>Object.entries(e).map((([e,t])=>`${e}:${t};`)).join("");return n.push(`::view-transition-old(root){${a(we(e,{...t,type:"outgoing"}))}}`,`::view-transition-new(root){${a(we(e,{...t,type:"incoming"}))}}`),n})(r,{backward:c(),duration:u.duration}).forEach((e=>i.sheet?.insertRule(e)));const s=document.documentElement.classList;s.add(Ee);let l=!1;const d=()=>{l||(n(a),l=!0,s.remove(Ee))},f=()=>{t(!1),i.remove(),s.remove(Ee)};try{t(!0);const e=document.startViewTransition(d);t(e),e.finished.finally(f)}catch(e){console.error(e),d(),f()}})),!1)};e.on("prev",r((t=>e.prev({...t,[$e]:!0})),{back:!0,cond:e=>e.index>0&&!((e.fragment??1)&&n.fragmentIndex>0)})),e.on("next",r((t=>e.next({...t,[$e]:!0})),{cond:t=>t.index+1<e.slides.length&&!(n.fragmentIndex+1<n.fragments.length)})),setTimeout((()=>{e.on("slide",r((t=>e.slide(t.index,{...t,[$e]:!0})),{cond:t=>{const n=e.slide();return t.index!==n&&(t.back=t.index<n,!0)}}))}),0),e.on("fragment",(e=>{n=e}))};let Ie;const Me=()=>(void 0===Ie&&(Ie="wakeLock"in navigator&&navigator.wakeLock),Ie),Oe=async()=>{const e=Me();if(e)try{return await e.request("screen")}catch(e){console.warn(e)}return null},Ae=async()=>{if(!Me())return;let e;const t=()=>{e&&"visible"===document.visibilityState&&Oe()};for(const e of["visibilitychange","fullscreenchange"])document.addEventListener(e,t);return e=await Oe(),e};((e=document.getElementById(":$p"))=>{(()=>{const e=p("view");a.dataset.bespokeView=e===l||e===c?e:""})();const t=(e=>{const t=p(e);return v({[e]:void 0}),t})("sync")||void 0;o.from(e,((...e)=>{const t=d.findIndex((e=>g()===e));return e.map((([e,n])=>e[t]&&n)).filter((e=>e))})([[1,1,0],re({key:t})],[[1,1,1],Z(e)],[[1,1,0],M],[[1,1,1],k],[[1,0,0],T()],[[1,1,1],O],[[1,1,1],ne({history:!1})],[[1,1,0],A()],[[1,1,0],P],[[1,0,0],ee],[[1,1,0],le()],[[1,0,0],C()],[[1,0,0],Te],[[1,1,1],$],[[1,1,0],Ae]))})()}();</script></body></html>