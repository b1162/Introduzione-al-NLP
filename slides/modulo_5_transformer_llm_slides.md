---
marp: true
theme: default
paginate: true
---

<!-- 
_class: lead
-->

# Transformer e Large Language Models ğŸ¤–
## La rivoluzione dell'NLP

<!--
In questa presentazione, introdurrÃ² i Transformer e i Large Language Models, che rappresentano la piÃ¹ grande rivoluzione nel campo dell'NLP degli ultimi anni. Questi modelli hanno completamente trasformato il modo in cui elaboriamo e generiamo linguaggio naturale, aprendo possibilitÃ  che fino a poco tempo fa sembravano fantascienza.

Domanda per la classe: Qualcuno ha mai interagito con ChatGPT o altri sistemi basati su LLM? Quali sono state le vostre impressioni?
-->

---

# Agenda ğŸ“‹

- Introduzione ai Transformer e LLM
- L'architettura Transformer in dettaglio
- Evoluzione dei Large Language Models
- CapacitÃ  e limitazioni
- Applicazioni pratiche
- Implementazione e considerazioni etiche

<!--
Oggi copriremo questi argomenti chiave, partendo dalle basi dei Transformer fino ad arrivare alle considerazioni etiche e alle sfide future. Il nostro obiettivo Ã¨ fornire una comprensione completa di queste tecnologie rivoluzionarie, sia dal punto di vista tecnico che pratico.

Questa Ã¨ una materia vasta, quindi sentitevi liberi di interrompere con domande in qualsiasi momento. CercherÃ² di mantenere la discussione il piÃ¹ interattiva possibile.
-->

---

# Introduzione ai Transformer e LLM ğŸŒŸ

- **Transformer**: Architettura rivoluzionaria introdotta nel 2017 ("Attention is All You Need")
- **Large Language Models (LLM)**: Modelli con miliardi di parametri addestrati su enormi corpora testuali
- Hanno trasformato radicalmente l'NLP e l'intero panorama dell'AI
- CapacitÃ  emergenti che vanno oltre l'addestramento esplicito

<!--
Nel 2017, un paper intitolato "Attention is All You Need" ha introdotto l'architettura Transformer, che ha rappresentato un punto di svolta fondamentale nell'NLP. Prima dei Transformer, i modelli RNN e LSTM erano lo stato dell'arte, ma soffrivano di limitazioni significative nell'elaborazione di sequenze lunghe e nella parallelizzazione.

I Transformer hanno risolto questi problemi con un meccanismo chiamato "self-attention", che permette di elaborare l'intera sequenza in parallelo e di catturare dipendenze a lungo termine in modo efficiente. Questo ha posto le basi per i Large Language Models, modelli enormi con miliardi di parametri che hanno mostrato capacitÃ  sorprendenti.

Una cosa particolarmente interessante Ã¨ che questi modelli mostrano "capacitÃ  emergenti" - abilitÃ  che non sono state esplicitamente programmate ma emergono dalla scala e complessitÃ  del modello.

Domanda per stimolare la riflessione: Secondo voi, cosa distingue i Transformer dalle architetture precedenti come RNN e LSTM? PerchÃ© hanno avuto un impatto cosÃ¬ rivoluzionario?
-->

---

# Attention is all you need

- https://arxiv.org/html/1706.03762v7

![alt text](images/attantion_is_all_you_need.png)


---

# Il meccanismo di self-attention ğŸ”


- Ogni elemento interagisce direttamente con tutti gli altri
- Cattura dipendenze a lungo termine in modo efficiente
- Permette elaborazione parallela (vs. sequenziale nelle RNN)
- Un esempio da visualizzare: https://github.com/jessevig/bertviz

<!--
Il cuore del Transformer Ã¨ il meccanismo di self-attention. A differenza delle RNN che elaborano il testo sequenzialmente, il self-attention permette a ogni parola di "prestare attenzione" direttamente a tutte le altre parole nella sequenza.

Nell'immagine, possiamo vedere come la parola "it" stia prestando attenzione principalmente alla parola "animal", indicando che il modello ha capito che "it" si riferisce ad "animal". Questo Ã¨ un esempio di come il self-attention catturi relazioni semantiche e sintattiche nel testo.

Per ogni parola, il modello calcola tre vettori: Query (cosa la parola sta cercando), Key (cosa la parola offre) e Value (il contenuto informativo della parola). L'attenzione viene calcolata come la compatibilitÃ  tra Query e Key, e poi usata per pesare i Value.

Questo meccanismo Ã¨ incredibilmente potente perchÃ© permette di catturare dipendenze a qualsiasi distanza con lo stesso costo computazionale, superando una limitazione fondamentale delle RNN.

Domanda per la classe: Pensate a una frase con riferimenti a lunga distanza. Come credete che un modello basato su RNN e uno basato su Transformer differiscano nell'elaborarla?
-->
---

![bg fit](images/ModalNet-21.png)


---

# Query, Key e Value ğŸ—ï¸

Per ogni elemento della sequenza, vengono calcolati tre vettori:

- **Query (Q)**: Cosa l'elemento "sta cercando"
- **Key (K)**: Cosa l'elemento "offre" agli altri
- **Value (V)**: Il contenuto informativo dell'elemento

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

<!--
Entriamo piÃ¹ nel dettaglio del meccanismo di self-attention. Per ogni parola nell'input, il modello calcola tre vettori attraverso trasformazioni lineari:

1. Query (Q): rappresenta ciÃ² che la parola "sta cercando" o "vuole sapere"
2. Key (K): rappresenta ciÃ² che la parola "offre" o "puÃ² dire" alle altre
3. Value (V): rappresenta il contenuto informativo effettivo della parola

Il calcolo dell'attenzione avviene in quattro passaggi:
1. Si calcola la compatibilitÃ  tra ogni Query e ogni Key con un prodotto scalare
2. Si divide per la radice quadrata della dimensione delle Key per stabilizzare i gradienti
3. Si applica softmax per ottenere pesi di attenzione normalizzati
4. Si calcola una media pesata dei Value usando questi pesi

La formula che vedete riassume questo processo. PuÃ² sembrare complessa, ma l'intuizione Ã¨ semplice: stiamo pesando l'importanza di ogni parola rispetto a ogni altra parola.

L'Essenza della Self-Attention nei Modelli Transformer
I Tre Vettori Fondamentali
Per ogni parola in una sequenza, il modello calcola:


Query (Q): Rappresenta "cosa la parola sta cercando" - la sua domanda al contesto

Key (K): Rappresenta "cosa la parola offre" - il suo identificativo per essere trovata

Value (V): Rappresenta "il contenuto informativo" - il messaggio che trasmette

Il Processo in Quattro Passaggi

Misura di rilevanza: Il prodotto scalare tra Q e K determina quanto ogni parola Ã¨ rilevante per le altre

Scalatura: Divisione per âˆšd_k per stabilizzare l'addestramento

Normalizzazione: La funzione softmax trasforma i punteggi in pesi probabilistici

Aggregazione: Combinazione pesata dei valori V in base ai pesi calcolati

La Formula Completa
La formula Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V racchiude l'intero processo:


QK^T: Calcola quanto ogni parola Ã¨ rilevante per le altre

/âˆšd_k: Stabilizza i valori per un apprendimento efficace

softmax(): Converte i punteggi in pesi probabilistici

Ã— V: Crea rappresentazioni contestuali combinando i valori in base ai pesi

Questa tecnica permette ai modelli di "vedere" le relazioni tra parole indipendentemente dalla loro distanza nella frase, catturando dipendenze complesse che erano inaccessibili ai modelli precedenti
-->

---

# Multi-Head Attention ğŸ§ 

![width:700px](https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png)

- Multiple "teste" di attenzione in parallelo
- Ogni testa puÃ² specializzarsi in diversi tipi di relazioni
- Cattura pattern diversi contemporaneamente

<!--
Il Transformer non utilizza una singola operazione di attenzione, ma diverse in parallelo, in quello che viene chiamato "multi-head attention". Ãˆ come avere piÃ¹ "teste" che guardano lo stesso input da prospettive diverse.

Ogni testa ha i propri parametri per calcolare Q, K e V, e quindi puÃ² specializzarsi nel catturare diversi tipi di relazioni. Ad esempio, una testa potrebbe focalizzarsi su relazioni sintattiche, un'altra su coreferenze, un'altra ancora su relazioni semantiche.

Nell'immagine, vediamo come le diverse teste di attenzione producano output diversi che vengono poi concatenati e proiettati per ottenere l'output finale.

Questo approccio multi-testa Ã¨ fondamentale per la potenza dei Transformer, perchÃ© permette di catturare simultaneamente diversi tipi di pattern linguistici.

Nella pratica, si osserva che diverse teste effettivamente si specializzano in diversi aspetti del linguaggio, emergendo naturalmente durante l'addestramento senza essere esplicitamente programmate per farlo.

Domanda per stimolare la discussione: In quali diversi tipi di relazioni linguistiche pensate che le diverse teste di attenzione potrebbero specializzarsi?
-->

---

# Positional Encoding ğŸ“

- I Transformer non hanno nozione intrinseca dell'ordine
- Si aggiungono encoding posizionali agli embedding

$$PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

$$PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

<!--
Una limitazione dell'architettura Transformer Ã¨ che, a differenza delle RNN, non ha una nozione intrinseca dell'ordine delle parole. Se mescolassimo le parole di una frase, il self-attention darebbe lo stesso risultato!

Per risolvere questo problema, si aggiungono "positional encoding" agli embedding di input. Questi encoding contengono informazioni sulla posizione di ogni token nella sequenza.

La formulazione originale utilizza funzioni sinusoidali di diverse frequenze, come mostrato nelle formule. Ogni posizione ha un pattern unico di valori, permettendo al modello di distinguere le diverse posizioni.

Questi encoding hanno proprietÃ  interessanti:
- Sono deterministici, non parametri apprendibili
- Ogni posizione ha un encoding unico
- La differenza relativa tra posizioni Ã¨ mantenuta indipendentemente dalla posizione assoluta
- Permettono potenzialmente di generalizzare a sequenze piÃ¹ lunghe di quelle viste durante l'addestramento

Domanda tecnica: PerchÃ© secondo voi si sono scelti encoding sinusoidali invece di semplici embedding posizionali apprendibili?
-->

---

# Architettura completa del Transformer ğŸ—ï¸

![width:700px](https://jalammar.github.io/images/t/the_transformer_encoder_decoder_stack.png)

- **Encoder**: Elabora l'input in parallelo
- **Decoder**: Genera l'output autoregressivamente
- Layer Normalization e Residual Connections
- Feed-Forward Networks

<!--
Ecco l'architettura completa del Transformer, composta da un encoder e un decoder.

L'encoder Ã¨ uno stack di blocchi identici (tipicamente 6), ciascuno contenente:
1. Multi-head self-attention
2. Feed-forward network (una rete a due strati con attivazione ReLU)
3. Layer normalization e residual connections

Il decoder Ã¨ anch'esso uno stack di blocchi identici, ma con un componente aggiuntivo:
1. Masked multi-head self-attention (per prevenire l'attenzione a posizioni future)
2. Multi-head attention sugli output dell'encoder (cross-attention)
3. Feed-forward network
4. Layer normalization e residual connections

Le residual connections (o skip connections) sono fondamentali per l'addestramento di reti profonde, permettendo ai gradienti di fluire piÃ¹ facilmente attraverso la rete.

La layer normalization normalizza gli input a ciascun sub-layer, calcolando media e varianza per ogni esempio individualmente.

Questa architettura ha dimostrato una straordinaria efficacia e versatilitÃ , diventando la base per praticamente tutti i modelli di linguaggio avanzati degli ultimi anni.

Domanda per la classe: Quali vantaggi pensate che questa architettura abbia rispetto a modelli precedenti come LSTM per compiti di traduzione automatica?
-->

---

# Da BERT a GPT: Paradigmi di pre-addestramento ğŸ”„

- **BERT** (2018): Encoder-only, bidirezionale
  - Masked Language Modeling (MLM)
  - Next Sentence Prediction (NSP)

- **GPT** (2018-2023): Decoder-only, unidirezionale
  - Predizione della parola successiva
  - Scaling massiccio: GPT-1 (117M) â†’ GPT-4 (trilioni?)

<!--
Dopo l'introduzione dell'architettura Transformer, sono emersi due paradigmi principali di pre-addestramento:

BERT (Bidirectional Encoder Representations from Transformers), introdotto da Google nel 2018, utilizza solo la parte encoder del Transformer. Ãˆ bidirezionale, considerando il contesto in entrambe le direzioni. Si pre-addestra con:
- Masked Language Modeling: predire parole mascherate casualmente
- Next Sentence Prediction: predire se due frasi sono consecutive

GPT (Generative Pre-trained Transformer), introdotto da OpenAI, utilizza solo la parte decoder. Ãˆ unidirezionale, considerando solo il contesto precedente. Si pre-addestra con la classica predizione della parola successiva.

L'evoluzione della serie GPT illustra la rapida scalata dei LLM:
- GPT-1 (2018): 117 milioni di parametri
- GPT-2 (2019): 1.5 miliardi di parametri
- GPT-3 (2020): 175 miliardi di parametri
- GPT-4 (2023): Dimensione non divulgata, ma stimata in trilioni di parametri

Domanda provocatoria: Secondo voi, Ã¨ piÃ¹ potente un approccio bidirezionale come BERT o unidirezionale come GPT? Quali sono i vantaggi e svantaggi di ciascuno?
-->

---

# BERT

![alt text 100% 40%](images/bert.jpg)
https://github.com/google-research/bert

---

# Scaling laws e emergent abilities ğŸ“ˆ

![width:700px](https://jalammar.github.io/images/gpt3/gpt3-training-data.png)

- **Scaling laws**: Relazioni prevedibili tra dimensioni, dati e performance
- **Emergent abilities**: CapacitÃ  che emergono improvvisamente oltre certe soglie
  - In-context learning
  - Chain-of-thought reasoning
  - Instruction following
  - Tool use

<!--
Una delle scoperte piÃ¹ significative nello sviluppo dei LLM Ã¨ l'esistenza di "scaling laws" (leggi di scala) che descrivono come le performance migliorano con l'aumentare delle dimensioni del modello, dei dati di addestramento e del compute.

Ricerche empiriche hanno mostrato che le performance seguono leggi di potenza prevedibili. Questo suggerisce che:
- Raddoppiare le dimensioni del modello produce miglioramenti prevedibili
- Esiste un trade-off ottimale tra dimensioni del modello e quantitÃ  di dati
- I miglioramenti continuano ben oltre le dimensioni precedentemente considerate pratiche

Ancora piÃ¹ sorprendente Ã¨ l'emergere di "emergent abilities" - capacitÃ  che non sono presenti in modelli piÃ¹ piccoli ma che emergono improvvisamente superando certe soglie dimensionali:
- In-context learning: apprendere da esempi forniti nel prompt
- Chain-of-thought reasoning: eseguire ragionamenti step-by-step
- Instruction following: seguire istruzioni complesse
- Tool use: utilizzare strumenti esterni quando appropriato

Queste capacitÃ  non sono state esplicitamente programmate ma emergono come proprietÃ  del sistema complesso.

Domanda filosofica per la classe: Cosa pensate che queste emergent abilities ci dicano sulla natura dell'intelligenza? Possiamo considerare questi modelli "intelligenti" in qualche senso?
-->

---

# Tecniche di addestramento avanzate ğŸ› ï¸

- **Instruction tuning**: Addestramento a seguire istruzioni
- **RLHF** (Reinforcement Learning from Human Feedback):
  - Modello di ricompensa basato su preferenze umane
  - Ottimizzazione tramite reinforcement learning
- **Mixture of Experts** (MoE):
  - Attivazione selettiva di "esperti" specializzati
  - Efficienza computazionale

<!--
L'evoluzione dei LLM non Ã¨ stata guidata solo dall'aumento delle dimensioni, ma anche da innovazioni nelle tecniche di addestramento.

L'instruction tuning consiste nell'addestrare il modello a seguire istruzioni in linguaggio naturale. Si crea un dataset di coppie (istruzione, risposta desiderata) e si fine-tuna il modello su questo dataset. Questo ha trasformato i LLM da semplici completatori di testo a assistenti interattivi.

RLHF (Reinforcement Learning from Human Feedback) Ã¨ una tecnica che utilizza il feedback umano per allineare i modelli con le preferenze umane:
1. Si addestra un modello di ricompensa basato su preferenze umane
2. Si utilizza questo modello per guidare l'ottimizzazione del LLM attraverso reinforcement learning
3. Il risultato Ã¨ un modello che genera risposte piÃ¹ utili, veritiere e sicure

Mixture of Experts (MoE) Ã¨ un'architettura che aumenta l'efficienza:
1. Invece di attivare l'intero modello per ogni input, si utilizzano "esperti" specializzati
2. Un "router" decide quali esperti attivare per ciascun input
3. Solo una frazione degli esperti viene attivata, riducendo il compute necessario

Domanda tecnica: Quali vantaggi pensate che RLHF offra rispetto al semplice fine-tuning supervisionato? PerchÃ© Ã¨ diventato cosÃ¬ importante per modelli come ChatGPT?
-->
---

# Mixture of Experts

![bg fit](images/MoE.jpg)

<!--
Dove entra la MoE (i riquadri rossi)
	â€¢	Il rettangolo rosso incornicia il punto in cui sostituiamo la FFN densa con una Mixture-of-Experts.
	â€¢	Meccanismo:
	1.	Router calcola, per ogni token, le probabilitÃ  di essere spedito a ciascun â€œespertoâ€.
	2.	k-Hot routing (di solito k = 1 â€“ 4) attiva solo pochi esperti â†’ sparse compute.
	3.	Gli esperti sono FFN indipendenti; i loro output vengono ri-pesati e sommati, poi passano allâ€™Add & Norm.

â¸»

PerchÃ© farlo
	â€¢	CapacitÃ  âˆ # esperti: possiamo arrivare a centinaia di miliardi di parametri senza far crescere proporzionalmente il FLOPs per token, perchÃ© ne usiamo solo una frazione.
	â€¢	Specializzazione: ogni esperto puÃ² imparare nicchie diverse del dominio, migliorando qualitÃ .
	â€¢	ScalabilitÃ  pratica: modelli come Switch Transformer, GLaM e PaLM sfruttano questa idea per addestrare reti â€œtrilionarieâ€ con costi vicini a un modello da poche decine di miliardi.

â¸»

Messaggio da portare a casa

â€œStesso Transformer, ma al posto di una sola FFN mettiamo unâ€™intera squadra di FFN, attivandone solo le poche piÃ¹ adatte a ogni token. CosÃ¬ moltiplichiamo la capacitÃ , non il costo.â€
-->

---

# Mixtral 8x7B


![alt text fit](images/smoe.png)

https://huggingface.co/blog/vtabbott/mixtral

<!--
Oggi parliamo di Mixtral 8 Ã— 7B e, attraverso di lei, di come funziona un Mixture-of-Experts (MoE).
In due righe: Mixtral Ã¨ un modello â€œsparse-MoEâ€ con 8 esperti per blocco feed-forward; ogni token viene instradato solo a 2 esperti, perciÃ² il calcolo resta leggero come in un modello da â‰ˆ 14 B parametri, ma la capacitÃ  totale supera i 45 B. Risultato: prestazioni simili o superiori a GPT-3.5 e Llama 2-70B, ma a costi dâ€™uso di un 13-14 B.  ï¿¼ ï¿¼ ï¿¼

â¸»

1. Che cosâ€™Ã¨ un Mixture-of-Experts

2. Idea base

Un MoE sostituisce la tradizionale rete feed-forward (FFN) di un Transformer con N reti FFN indipendenti (â€œespertiâ€) piÃ¹ un router che decide, token per token, quali esperti attivare. Il routing piÃ¹ usato Ã¨ top-2: per ogni token si scelgono i 2 esperti con punteggio piÃ¹ alto e si combina il loro output.  ï¿¼ ï¿¼ ï¿¼

2.2 Vantaggi
	â€¢	CapacitÃ  quasi illimitata: basta aggiungere esperti per aumentare i parametri totali.
	â€¢	Costo costante: se si attivano solo k esperti su N, il numero di FLOPs per token cresce di k/N â‰ˆ costante.
	â€¢	Specializzazione: gli esperti competono e si specializzano su lingue, codice, logica, ecc.  ï¿¼ ï¿¼

â¸»

3. Mixtral come esempio pratico

Caratteristica	Dettaglio	Collegamento al concetto MoE
8 esperti / MLP	Ogni blocco FFN Ã¨ replicato 8 volte â†’ 45 B parametri totali.  ï¿¼	Aumenta la capacitÃ .
Top-2 routing	Per ogni token si attivano solo 2 esperti â†’ â‰ˆ 12,9 B parametri attivi.  ï¿¼ ï¿¼	Mantiene costo computazionale di un 14 B.
Router leggero	Ãˆ un piccolo layer lineare che produce le probabilitÃ  di scelta.  ï¿¼	Realizza la sparse activation.
Contesto 32 k	Usa Sliding-Window Attention e GQA per gestire sequenze lunghe.  ï¿¼ ï¿¼	Mostra come ottimizzare altre parti del Transformer.
Licenza Apache-2.0	Pesi liberamente scaricabili e ri-usabili.  [oai_citation_attribution:15â€¡Mistral AI	Frontier AI in your hands](https://mistral.ai/news/mixtral-of-experts?utm_source=chatgpt.com) ï¿¼


â¸»

4. Giro di un token in Mixtral (passo-passo)
	1.	Embedding & attenzione: identico a Mistral-7B, ma con Sliding-Window per 32 k token.  ï¿¼
	2.	Router calcola 8 logit â†’ softmax â†’ sceglie i 2 migliori (capacitÃ  factor 1.0 â€“ 1.25).  ï¿¼
	3.	Spedizione: il token viene copiato nei due sotto-batch degli esperti prescelti.
	4.	Calcolo esperti: ciascun esperto Ã¨ una FFN tipica (linear-â†’ReLU-â†’linear).
	5.	Ricomposizione: gli output dei 2 esperti sono ricombinati (pesi del router) e passano al blocco successivo.
	6.	Ripeti per ogni layer: gli esperti scelti possono cambiare a ogni passo, permettendo specializzazione dinamica.  ï¿¼

â¸»

5. Performance e costi
	â€¢	Benchmark: Mixtral batte Llama 2-70B su quasi tutti i task e pareggia o supera GPT-3.5 su molti benchmark pubblici.  ï¿¼ ï¿¼
	â€¢	VelocitÃ : ~6 Ã— piÃ¹ rapida di Llama 2-70B allâ€™inferenza su stessa GPU (perchÃ© calcola solo 2 FFN invece di una FFN grande).  ï¿¼
	â€¢	Memoria: serve RAM/GPU per tutti gli esperti (â‰ˆ 70 B come footprint), ma lâ€™active-set resta piccolo â†’ ottimo trade-off su GPU H100/â€½80 GB.  ï¿¼
	â€¢	Costo-efficacia: migliore â€œquality-per-dollarâ€ tra i modelli open weight oggi disponibili.  ï¿¼

â¸»

6. Altre note utili
	â€¢	Checkpoint rilasciati
	â€¢	Mixtral-8Ã—7B-v0.1 (base) â€“ pre-training puro.
	â€¢	Mixtral-8Ã—7B-Instruct-v0.1 â€“ fine-tune SFT + DPO per chat.  ï¿¼
	â€¢	Tokenizzatore byte-fallback: mai OOV, utile per lingue europee e codice.  ï¿¼
	â€¢	Tooling: ottimizzato per vLLM, Megablocks e altre librerie MoE-aware.  ï¿¼

â¸»

7. Take-away

MoE = capacitÃ  elastica + calcolo costante.
Mixtral dimostra che, scegliendo pochi esperti per token, possiamo avere un modello che â€œpensaâ€ come un 45 B-param ma gira (e si fine-tuna) come un 14 B. Questo rende i grandi LLM di qualitÃ  GPT-3.5 accessibili anche a chi ha una sola GPU di fascia alta.
-->

---

# Modelli multimodali ğŸ–¼ï¸ğŸ”ŠğŸ“

- **Vision-Language Models**: Comprensione visiva integrata (GPT-4V, Gemini)
- **Generazione multimediale**: Immagini (DALL-E), audio, video
- **Modelli unificati**: Integrazione seamless di diverse modalitÃ 

<!--
L'evoluzione piÃ¹ recente dei LLM Ã¨ l'estensione verso capacitÃ  multimodali, integrando comprensione e generazione di diverse modalitÃ  oltre al testo.

Modelli come GPT-4V, Gemini e Claude Opus integrano capacitÃ  di comprensione visiva:
- Possono "vedere" e analizzare immagini fornite dall'utente
- Rispondono a domande su contenuti visivi
- Ragionano su diagrammi, grafici e visualizzazioni
- Possono descrivere scene complesse con dettaglio e accuratezza

Alcuni modelli estendono le capacitÃ  generative oltre il testo:
- Generazione di immagini: modelli come DALL-E, Midjourney e Stable Diffusion
- Generazione di audio: conversione text-to-speech e generazione musicale
- Generazione di video: creazione di brevi clip video da descrizioni testuali

La frontiera attuale Ã¨ rappresentata da modelli che integrano seamlessly diverse modalitÃ  in un unico sistema, mantenendo coerenza cross-modale e permettendo interazioni naturali che combinano diverse modalitÃ .

Domanda creativa: Quali applicazioni innovative pensate che potrebbero emergere dalla combinazione di capacitÃ  linguistiche e visive in un unico modello?
-->

---

# CapacitÃ  fondamentali dei LLM ğŸ’ª

- **Comprensione contestuale**: Disambiguazione, coreference resolution
- **Generazione fluente e coerente**: Coerenza locale e globale, adattamento stilistico
- **In-context learning**: Few-shot learning, adattamento a nuovi compiti
- **Ragionamento e problem solving**: Chain-of-thought, decomposizione di problemi

<!--
I LLM hanno dimostrato capacitÃ  sorprendenti che hanno trasformato le aspettative sul potenziale dell'intelligenza artificiale.

La comprensione contestuale permette ai modelli di interpretare correttamente parole e frasi ambigue basandosi sul contesto, identificare a cosa si riferiscono pronomi e altre espressioni anaforiche, e cogliere sottili differenze di significato.

La generazione fluente e coerente si manifesta nella capacitÃ  di mantenere coerenza grammaticale e semantica a livello di frase, coerenza tematica e narrativa attraverso paragrafi, e adattarsi a diversi stili di scrittura e registri.

L'in-context learning Ã¨ la sorprendente capacitÃ  di apprendere da esempi forniti nel prompt, identificare e applicare pattern, e adattarsi a nuovi compiti senza fine-tuning esplicito.

Le capacitÃ  di ragionamento emergono particolarmente quando il modello viene istruito a "pensare passo dopo passo", permettendo di eseguire ragionamenti step-by-step, verificare la coerenza delle proprie conclusioni, e decomporre problemi complessi.

Domanda per stimolare la riflessione: Quale di queste capacitÃ  vi sorprende di piÃ¹? Quale pensate sia la piÃ¹ difficile da replicare per un sistema artificiale?
-->

---

# Limitazioni fondamentali ğŸš«

- **Allucinazioni**: Generazione di contenuti plausibili ma fattuali errati
- **Bias e stereotipi**: Perpetuazione di pregiudizi presenti nei dati
- **Limitazioni di contesto**: Finestra di contesto finita, degradazione dell'attenzione
- **Mancanza di comprensione profonda**: Comprensione simbolica limitata, grounding

<!--
Nonostante le loro impressionanti capacitÃ , i LLM presentano limitazioni significative.

Le allucinazioni sono una delle piÃ¹ problematiche: i modelli possono generare contenuti che sembrano plausibili ma sono fattuali errati, inventare dettagli, fonti o citazioni inesistenti, e combinare erroneamente informazioni relative a entitÃ  diverse.

I bias e stereotipi presenti nei dati di addestramento possono essere perpetuati o amplificati, manifestandosi come rappresentazioni stereotipate basate su genere, etnia, etÃ , predominanza di prospettive occidentali, e tendenza a supportare credenze preesistenti.

I LLM hanno limitazioni pratiche nella gestione di contesti lunghi, con una finestra di contesto finita, attenzione meno efficace per token distanti, e difficoltÃ  nel mantenere coerenza in conversazioni molto lunghe.

Infine, mancano di una comprensione profonda nel senso umano, con limitata capacitÃ  di manipolazione simbolica e ragionamento astratto, mancanza di connessione diretta con il mondo fisico, e comprensione imperfetta di stati mentali e intenzioni.

Domanda critica: Quali di queste limitazioni pensate siano fondamentali e quali potrebbero essere superate con futuri sviluppi tecnologici?
-->

---

# Applicazioni pratiche ğŸ”§

- **Automazione documentale**: Generazione, analisi, estrazione, summarization
- **Assistenza alla scrittura**: Brainstorming, drafting, editing, traduzione
- **Assistenti virtuali**: Customer service avanzato, esperienze conversazionali
- **Supporto decisionale**: Analisi di dati testuali, sintesi di evidenze
- **CreativitÃ  aumentata**: Ideazione creativa, content creation scalabile

<!--
I Transformer e i LLM stanno trasformando numerosi settori con applicazioni pratiche che spaziano dall'automazione di compiti ripetitivi alla creazione di nuove forme di interazione.

Nell'automazione documentale, eccellono nella creazione di bozze di contratti e report, revisione e editing di documenti esistenti, estrazione di informazioni chiave, e creazione di sintesi mantenendo i punti essenziali.

Come assistenti alla scrittura, fungono da collaboratori intelligenti per brainstorming e ideazione, assistenza nella creazione e raffinamento di contenuti, riformulazione per diversi pubblici, e traduzione che preserva sfumature e contesto culturale.

Gli assistenti virtuali basati su LLM offrono comprensione di richieste complesse, risposte contestuali considerando la storia conversazionale, risoluzione end-to-end di casi, e conversazioni fluide e coerenti.

Nel supporto decisionale, aiutano ad analizzare notizie e social media per trend emergenti, sintetizzare feedback e recensioni, monitorare attivitÃ  dei concorrenti, e raccogliere e organizzare informazioni rilevanti per decisioni complesse.

Infine, stanno ridefinendo i processi creativi, fungendo da collaboratori che amplificano la creativitÃ  umana attraverso generazione di concept e storie, esplorazione di diverse versioni di un'idea, e creazione di contenuti adattati a diversi segmenti e piattaforme.

Domanda pratica: Quali di queste applicazioni pensate potrebbero avere il maggiore impatto nel vostro settore o campo di interesse?
-->

---

# Implementazione pratica ğŸ–¥ï¸

- **Approcci di deployment**:
  - API commerciali vs modelli open-source
  - On-premise vs cloud vs edge
- **Ottimizzazione e personalizzazione**:
  - Fine-tuning e adattamento al dominio
  - Retrieval-Augmented Generation (RAG)
- **Integrazione nei flussi di lavoro**:
  - Connessione con sistemi esistenti
  - Human-in-the-loop approaches

<!--
L'implementazione pratica di LLM in contesti reali richiede considerazioni tecniche, strategiche e organizzative.

Per il deployment, esistono diversi approcci:
- API commerciali (OpenAI, Anthropic) offrono rapida implementazione senza costi infrastrutturali, ma con potenziali problemi di privacy
- Modelli open-source (Llama, Mistral) offrono controllo totale e privacy dei dati, ma richiedono infrastruttura e competenze tecniche
- Approcci ibridi combinano i vantaggi di entrambi

Per ottimizzazione e personalizzazione:
- Il fine-tuning completo aggiorna tutti i parametri del modello su dati specifici
- Tecniche efficienti come LoRA aggiornano solo matrici di basso rango
- Retrieval-Augmented Generation combina LLM con sistemi di recupero documentale per accedere a informazioni proprietarie

L'integrazione nei flussi di lavoro richiede:
- Connessione con sistemi CRM, ERP, knowledge management
- Approcci human-in-the-loop che mantengono gli umani nel processo decisionale
- Processi di revisione umana per output critici

Domanda tecnica: Quali fattori considerereste piÃ¹ importanti nella scelta tra API commerciali e modelli open-source per un'implementazione aziendale?
-->

---

# Conclusione ğŸ¯

- I Transformer hanno rivoluzionato l'NLP con il meccanismo di self-attention
- I LLM hanno mostrato capacitÃ  emergenti sorprendenti con lo scaling
- Nonostante limitazioni come allucinazioni e bias, le applicazioni pratiche sono trasformative
- L'implementazione efficace richiede considerazioni tecniche, strategiche ed etiche
- Il futuro promette ulteriori progressi in allineamento, efficienza e multimodalitÃ 

<!--
In conclusione, i Transformer e i Large Language Models rappresentano una delle piÃ¹ significative rivoluzioni tecnologiche del nostro tempo, con implicazioni profonde che si estendono ben oltre il campo del Natural Language Processing.

L'architettura Transformer, con il suo innovativo meccanismo di self-attention, ha superato le limitazioni delle architetture precedenti, permettendo l'elaborazione parallela e la cattura efficiente di dipendenze a lungo termine.

I Large Language Models hanno mostrato una rapida evoluzione in termini di dimensioni e capacitÃ , con emergent abilities sorprendenti che emergono dalla scala e complessitÃ .

Nonostante limitazioni significative come allucinazioni, bias e mancanza di comprensione profonda, le applicazioni pratiche stanno trasformando numerosi settori, dall'automazione documentale alla creativitÃ  aumentata.

L'implementazione efficace richiede considerazioni attente su approcci di deployment, ottimizzazione, integrazione nei flussi di lavoro e framework di governance.

Guardando al futuro, frontiere di ricerca come allineamento, efficienza, multimodalitÃ  e embodiment promettono di affrontare limitazioni attuali e aprire nuove possibilitÃ .

Domanda finale per la classe: Come pensate che queste tecnologie evolveranno nei prossimi 5-10 anni? Quali impatti prevedete sulla societÃ  e sul vostro futuro professionale?
-->

---

# Grazie per l'attenzione! ğŸ™

## Domande?

<!--
Grazie per la vostra attenzione e partecipazione! Spero che questa presentazione vi abbia fornito una comprensione chiara dei Transformer e dei Large Language Models, sia dal punto di vista tecnico che delle loro implicazioni pratiche ed etiche.
-->